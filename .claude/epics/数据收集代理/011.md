---
name: 数据验证和异常处理机制
type: task
epic: 数据收集代理
status: open
priority: 2
created: 2025-09-25T20:20:35Z
estimated: 8 days
assigned: [待分配]
parallelizable: true
dependencies: ["010"]
---

# 任务: 数据验证和异常处理机制

## 任务描述
实现完整的数据验证系统和异常数据处理机制，包括完整性检查、异常检测、自动修复和告警机制。通过智能化的验证和异常处理流程，确保数据的准确性和可靠性，为交易决策提供高质量的数据支持。

## 技术要求

### 核心架构设计

#### Data Validation System 组件
- **完整性检查**: 验证数据的完整性和一致性
- **异常检测**: 智能检测数据异常和异常模式
- **自动修复**: 基于规则的自动数据修复
- **告警机制**: 实时异常告警和通知
- **异常记录**: 详细的异常记录和分析

#### 技术栈
- **核心库**: Python 3.8+ + AsyncIO
- **统计分析**: SciPy + StatsModels
- **机器学习**: Scikit-learn (异常检测)
- **数据库**: PostgreSQL + Redis
- **监控**: Prometheus + AlertManager

### 实现架构

```python
# src/validation/data_validator.py
import asyncio
import time
from typing import Dict, List, Optional, Any, Callable, Union, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from decimal import Decimal
from enum import Enum
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import logging
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_, or_
import json
from collections import defaultdict, deque
import re

from ..core.config import config
from ..core.logger import get_logger
from ..core.metrics import metrics
from ..core.exceptions import ValidationError, DataIntegrityError
from ..storage.postgresql import get_db_session
from ..storage.redis import redis_client
from ..models.validation import ValidationRule, ValidationReport, AnomalyRecord, RepairRecord
from ..models.market_data import OHLCV, Ticker, OrderBook, Trade

class ValidationLevel(Enum):
    """验证级别枚举"""
    STRICT = "strict"      # 严格验证，任何异常都拒绝
    NORMAL = "normal"      # 正常验证，警告但接受
    RELAXED = "relaxed"    # 宽松验证，只记录严重异常

class AnomalyType(Enum):
    """异常类型枚举"""
    OUTLIER = "outlier"           # 离群值
    MISSING_DATA = "missing_data"  # 缺失数据
    INCONSISTENT = "inconsistent"  # 不一致数据
    DUPLICATE = "duplicate"        # 重复数据
    FORMAT_ERROR = "format_error"  # 格式错误
    RANGE_VIOLATION = "range_violation"  # 范围违规
    TIMELINE_GAP = "timeline_gap"  # 时间线断裂
    VOLUME_SPIKE = "volume_spike"  # 成交量异常
    PRICE_ANOMALY = "price_anomaly"  # 价格异常

class AlertSeverity(Enum):
    """告警严重程度枚举"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class RepairAction(Enum):
    """修复动作枚举"""
    INTERPOLATE = "interpolate"      # 插值修复
    FILL_FORWARD = "fill_forward"    # 向前填充
    FILL_BACKWARD = "fill_backward"  # 向后填充
    MEAN_IMPUTATION = "mean_imputation"  # 均值填充
    MEDIAN_IMPUTATION = "median_imputation"  # 中位数填充
    REMOVE_RECORD = "remove_record"  # 删除记录
    FLAG_RECORD = "flag_record"      # 标记记录
    NO_ACTION = "no_action"          # 不处理

@dataclass
class ValidationResult:
    """验证结果"""
    is_valid: bool
    validation_score: float
    anomalies: List[Dict[str, Any]]
    warnings: List[str]
    errors: List[str]
    recommendations: List[str]

@dataclass
class AnomalyDetection:
    """异常检测结果"""
    anomaly_type: AnomalyType
    severity: AlertSeverity
    confidence: float
    description: str
    affected_records: List[int]
    suggested_action: RepairAction

@dataclass
class RepairResult:
    """修复结果"""
    success: bool
    repaired_count: int
    failed_repairs: int
    repair_methods: Dict[str, int]
    repair_stats: Dict[str, Any]

class DataValidator:
    """数据验证器"""

    def __init__(self):
        self.logger = get_logger("data_validator")
        self.validation_level = ValidationLevel.NORMAL
        self.anomaly_detector = None
        self.scaler = StandardScaler()
        self.cache_ttl = 300  # 5分钟缓存
        self.alert_thresholds = {
            'anomaly_rate': 0.05,     # 5%异常率
            'missing_rate': 0.02,    # 2%缺失率
            'error_rate': 0.01       # 1%错误率
        }
        self._running = False
        self._monitoring_task = None

        # 历史数据窗口
        self.history_window = 1000
        self.price_history = defaultdict(lambda: deque(maxlen=self.history_window))
        self.volume_history = defaultdict(lambda: deque(maxlen=self.history_window))

        # 异常检测模型
        self.isolation_forest = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_jobs=-1
        )

        # 统计信息
        self.validation_stats = {
            'total_validated': 0,
            'total_anomalies': 0,
            'total_repairs': 0,
            'total_alerts': 0,
            'avg_validation_score': 0.0,
            'anomaly_rate': 0.0
        }

    async def initialize(self):
        """初始化数据验证器"""
        try:
            # 加载历史数据用于训练模型
            await self._load_historical_data()

            # 训练异常检测模型
            await self._train_anomaly_detection_model()

            # 启动监控任务
            self._running = True
            self._monitoring_task = asyncio.create_task(self._validation_monitor_loop())

            self.logger.info("Data Validator 初始化完成")

        except Exception as e:
            self.logger.error(f"Data Validator 初始化失败: {e}")
            raise

    async def _load_historical_data(self):
        """加载历史数据"""
        try:
            # 从数据库加载历史数据用于训练
            async with get_db_session() as session:
                # 加载过去7天的OHLCV数据
                start_date = datetime.now() - timedelta(days=7)
                result = await session.execute(
                    select(OHLCV).where(
                        OHLCV.timestamp >= start_date
                    ).order_by(OHLCV.timestamp)
                )

                # 存储历史数据
                for record in result.scalars().all():
                    key = f"{record.exchange}_{record.symbol}"
                    self.price_history[key].append(record.close)
                    self.volume_history[key].append(record.volume)

        except Exception as e:
            self.logger.error(f"加载历史数据失败: {e}")

    async def _train_anomaly_detection_model(self):
        """训练异常检测模型"""
        try:
            # 准备训练数据
            all_prices = []
            all_volumes = []

            for key, prices in self.price_history.items():
                if len(prices) >= 50:  # 需要足够的数据
                    # 价格变化率
                    price_changes = np.diff(prices) / prices[:-1]
                    all_prices.extend(price_changes)

                    # 成交量变化率
                    volumes = list(self.volume_history[key])
                    if len(volumes) >= len(prices):
                        volume_changes = np.diff(volumes) / volumes[:-1]
                        all_volumes.extend(volume_changes)

            if len(all_prices) >= 100:
                # 合并特征
                features = []
                for i in range(min(len(all_prices), len(all_volumes))):
                    features.append([all_prices[i], all_volumes[i]])

                # 训练模型
                features_array = np.array(features)
                scaled_features = self.scaler.fit_transform(features_array)
                self.isolation_forest.fit(scaled_features)

                self.logger.info(f"异常检测模型训练完成，使用 {len(features)} 条样本")

        except Exception as e:
            self.logger.error(f"训练异常检测模型失败: {e}")

    async def validate_ohlcv_data(self, data: List[Dict], exchange: str, symbol: str) -> ValidationResult:
        """验证OHLCV数据"""
        try:
            start_time = time.time()

            # 转换为DataFrame
            df = pd.DataFrame(data)

            # 完整性检查
            integrity_result = await self._check_data_integrity(df, 'ohlcv')

            # 异常检测
            anomaly_result = await self._detect_anomalies(df, 'ohlcv', exchange, symbol)

            # 数据一致性验证
            consistency_result = await self._check_data_consistency(df, 'ohlcv')

            # 综合验证结果
            validation_score = self._calculate_validation_score(
                integrity_result, anomaly_result, consistency_result
            )

            # 生成建议
            recommendations = await self._generate_recommendations(
                integrity_result, anomaly_result, consistency_result
            )

            # 更新统计信息
            await self._update_validation_stats(
                len(df), len(anomaly_result), validation_score
            )

            # 记录验证时间
            validation_time = time.time() - start_time

            # 记录指标
            metrics.record_validation_metrics(
                data_type='ohlcv',
                record_count=len(df),
                validation_score=validation_score,
                anomaly_count=len(anomaly_result),
                validation_time=validation_time
            )

            # 检查是否需要告警
            if validation_score < 0.8 or len(anomaly_result) > 0:
                await self._trigger_validation_alert(
                    exchange, symbol, validation_score, anomaly_result
                )

            return ValidationResult(
                is_valid=validation_score >= 0.8,
                validation_score=validation_score,
                anomalies=anomaly_result,
                warnings=[],
                errors=[],
                recommendations=recommendations
            )

        except Exception as e:
            self.logger.error(f"验证OHLCV数据失败: {e}")
            return ValidationResult(
                is_valid=False,
                validation_score=0.0,
                anomalies=[],
                warnings=[],
                errors=[str(e)],
                recommendations=[]
            )

    async def _check_data_integrity(self, df: pd.DataFrame, data_type: str) -> Dict[str, Any]:
        """检查数据完整性"""
        try:
            integrity_report = {
                'total_records': len(df),
                'missing_records': 0,
                'duplicate_records': 0,
                'timeline_gaps': 0,
                'format_errors': 0,
                'integrity_score': 1.0
            }

            if df.empty:
                integrity_report['integrity_score'] = 0.0
                return integrity_report

            # 检查缺失值
            missing_counts = df.isnull().sum()
            integrity_report['missing_records'] = missing_counts.sum()

            # 检查重复记录
            if 'timestamp' in df.columns:
                duplicate_count = df.duplicated(subset=['timestamp']).sum()
                integrity_report['duplicate_records'] = duplicate_count

            # 检查时间线断裂
            if 'timestamp' in df.columns and len(df) > 1:
                time_diffs = df['timestamp'].diff().dropna()
                expected_interval = time_diffs.median()

                # 检查异常的时间间隔
                gap_threshold = expected_interval * 3  # 3倍于正常间隔
                gaps = time_diffs[time_diffs > gap_threshold]
                integrity_report['timeline_gaps'] = len(gaps)

            # 检查格式错误
            if data_type == 'ohlcv':
                format_errors = 0

                # 检查数值字段
                numeric_fields = ['open', 'high', 'low', 'close', 'volume']
                for field in numeric_fields:
                    if field in df.columns:
                        non_numeric = df[field].apply(lambda x: not isinstance(x, (int, float)))
                        format_errors += non_numeric.sum()

                integrity_report['format_errors'] = format_errors

            # 计算完整性评分
            total_issues = (
                integrity_report['missing_records'] +
                integrity_report['duplicate_records'] +
                integrity_report['timeline_gaps'] +
                integrity_report['format_errors']
            )

            integrity_report['integrity_score'] = max(
                0.0, 1.0 - (total_issues / max(integrity_report['total_records'], 1))
            )

            return integrity_report

        except Exception as e:
            self.logger.error(f"检查数据完整性失败: {e}")
            return {'integrity_score': 0.0, 'error': str(e)}

    async def _detect_anomalies(self, df: pd.DataFrame, data_type: str, exchange: str, symbol: str) -> List[Dict[str, Any]]:
        """检测数据异常"""
        try:
            anomalies = []

            if df.empty or len(df) < 5:
                return anomalies

            # 基于统计的异常检测
            statistical_anomalies = await self._detect_statistical_anomalies(df, data_type)
            anomalies.extend(statistical_anomalies)

            # 基于机器学习的异常检测
            if self.isolation_forest is not None:
                ml_anomalies = await self._detect_ml_anomalies(df, data_type, exchange, symbol)
                anomalies.extend(ml_anomalies)

            # 基于规则的异常检测
            rule_anomalies = await self._detect_rule_based_anomalies(df, data_type)
            anomalies.extend(rule_anomalies)

            # 去重和排序
            unique_anomalies = []
            seen_anomalies = set()

            for anomaly in anomalies:
                anomaly_key = (
                    anomaly.get('type'),
                    anomaly.get('record_index'),
                    anomaly.get('field')
                )
                if anomaly_key not in seen_anomalies:
                    seen_anomalies.add(anomaly_key)
                    unique_anomalies.append(anomaly)

            # 按严重程度排序
            severity_order = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}
            unique_anomalies.sort(
                key=lambda x: severity_order.get(x.get('severity', 'low'), 0),
                reverse=True
            )

            return unique_anomalies

        except Exception as e:
            self.logger.error(f"检测数据异常失败: {e}")
            return []

    async def _detect_statistical_anomalies(self, df: pd.DataFrame, data_type: str) -> List[Dict[str, Any]]:
        """统计异常检测"""
        anomalies = []

        try:
            if data_type == 'ohlcv':
                # 价格异常检测
                for field in ['open', 'high', 'low', 'close']:
                    if field in df.columns:
                        # Z-score检测
                        z_scores = np.abs(stats.zscore(df[field].dropna()))
                        outliers = np.where(z_scores > 3)[0]  # 3个标准差

                        for idx in outliers:
                            anomalies.append({
                                'type': 'price_anomaly',
                                'field': field,
                                'record_index': int(idx),
                                'value': float(df.iloc[idx][field]),
                                'z_score': float(z_scores[idx]),
                                'severity': 'high' if z_scores[idx] > 5 else 'medium',
                                'confidence': min(float(z_scores[idx] / 5), 1.0),
                                'description': f"{field} 值异常偏离 {z_scores[idx]:.2f} 个标准差"
                            })

                # 成交量异常检测
                if 'volume' in df.columns:
                    volume_z_scores = np.abs(stats.zscore(df['volume'].dropna()))
                    volume_outliers = np.where(volume_z_scores > 3)[0]

                    for idx in volume_outliers:
                        anomalies.append({
                            'type': 'volume_spike',
                            'field': 'volume',
                            'record_index': int(idx),
                            'value': float(df.iloc[idx]['volume']),
                            'z_score': float(volume_z_scores[idx]),
                            'severity': 'medium' if volume_z_scores[idx] > 5 else 'low',
                            'confidence': min(float(volume_z_scores[idx] / 5), 1.0),
                            'description': f"成交量异常 {volume_z_scores[idx]:.2f} 个标准差"
                        })

        except Exception as e:
            self.logger.error(f"统计异常检测失败: {e}")

        return anomalies

    async def _detect_ml_anomalies(self, df: pd.DataFrame, data_type: str, exchange: str, symbol: str) -> List[Dict[str, Any]]:
        """机器学习异常检测"""
        anomalies = []

        try:
            if self.isolation_forest is None:
                return anomalies

            if data_type == 'ohlcv' and len(df) >= 10:
                # 准备特征
                features = []
                for i in range(len(df)):
                    row = df.iloc[i]

                    # 价格变化特征
                    if i > 0:
                        prev_row = df.iloc[i-1]
                        price_change = (row['close'] - prev_row['close']) / prev_row['close']
                        volume_change = (row['volume'] - prev_row['volume']) / prev_row['volume'] if prev_row['volume'] > 0 else 0
                    else:
                        price_change = 0
                        volume_change = 0

                    # 价格范围特征
                    if row['high'] > row['low']:
                        price_range = (row['high'] - row['low']) / row['low']
                    else:
                        price_range = 0

                    features.append([price_change, volume_change, price_range])

                # 检测异常
                features_array = np.array(features)
                scaled_features = self.scaler.transform(features_array)
                anomaly_scores = self.isolation_forest.decision_function(scaled_features)
                anomaly_labels = self.isolation_forest.predict(scaled_features)

                # 识别异常点
                for i, (label, score) in enumerate(zip(anomaly_labels, anomaly_scores)):
                    if label == -1:  # 异常点
                        severity = 'high' if score < -0.3 else 'medium'
                        confidence = min(abs(score), 1.0)

                        anomalies.append({
                            'type': 'ml_anomaly',
                            'field': 'multi_feature',
                            'record_index': int(i),
                            'value': float(score),
                            'anomaly_score': float(score),
                            'severity': severity,
                            'confidence': confidence,
                            'description': f"机器学习检测到异常，异常分数: {score:.3f}"
                        })

        except Exception as e:
            self.logger.error(f"机器学习异常检测失败: {e}")

        return anomalies

    async def _detect_rule_based_anomalies(self, df: pd.DataFrame, data_type: str) -> List[Dict[str, Any]]:
        """基于规则的异常检测"""
        anomalies = []

        try:
            if data_type == 'ohlcv':
                # OHLC一致性规则
                invalid_ohlc = df[
                    (df['high'] < df['low']) |
                    (df['high'] < df['open']) |
                    (df['high'] < df['close']) |
                    (df['low'] > df['open']) |
                    (df['low'] > df['close'])
                ]

                for idx, row in invalid_ohlc.iterrows():
                    anomalies.append({
                        'type': 'inconsistent',
                        'field': 'ohlc',
                        'record_index': int(idx),
                        'value': {
                            'open': float(row['open']),
                            'high': float(row['high']),
                            'low': float(row['low']),
                            'close': float(row['close'])
                        },
                        'severity': 'high',
                        'confidence': 1.0,
                        'description': 'OHLC数据不一致'
                    })

                # 负价格检查
                negative_prices = df[
                    (df['open'] <= 0) |
                    (df['high'] <= 0) |
                    (df['low'] <= 0) |
                    (df['close'] <= 0)
                ]

                for idx, row in negative_prices.iterrows():
                    anomalies.append({
                        'type': 'range_violation',
                        'field': 'price',
                        'record_index': int(idx),
                        'value': float(row['close']),
                        'severity': 'critical',
                        'confidence': 1.0,
                        'description': '检测到负价格'
                    })

                # 负成交量检查
                negative_volume = df[df['volume'] < 0]
                for idx, row in negative_volume.iterrows():
                    anomalies.append({
                        'type': 'range_violation',
                        'field': 'volume',
                        'record_index': int(idx),
                        'value': float(row['volume']),
                        'severity': 'high',
                        'confidence': 1.0,
                        'description': '检测到负成交量'
                    })

        except Exception as e:
            self.logger.error(f"基于规则的异常检测失败: {e}")

        return anomalies

    async def _check_data_consistency(self, df: pd.DataFrame, data_type: str) -> Dict[str, Any]:
        """检查数据一致性"""
        try:
            consistency_report = {
                'consistency_score': 1.0,
                'inconsistencies': []
            }

            if df.empty:
                consistency_report['consistency_score'] = 0.0
                return consistency_report

            if data_type == 'ohlcv':
                # 检查时间序列连续性
                if 'timestamp' in df.columns:
                    sorted_df = df.sort_values('timestamp')
                    time_diffs = sorted_df['timestamp'].diff().dropna()

                    if len(time_diffs) > 0:
                        # 检查时间间隔的一致性
                        interval_std = time_diffs.std()
                        interval_mean = time_diffs.mean()

                        if interval_mean > 0:
                            coefficient_of_variation = interval_std / interval_mean
                            if coefficient_of_variation > 0.5:  # 变异系数过高
                                consistency_report['consistency_score'] *= 0.8
                                consistency_report['inconsistencies'].append(
                                    f"时间间隔变异系数过高: {coefficient_of_variation:.2f}"
                                )

                # 检查数值范围的一致性
                numeric_fields = ['open', 'high', 'low', 'close', 'volume']
                for field in numeric_fields:
                    if field in df.columns:
                        values = df[field].dropna()
                        if len(values) > 0:
                            # 检查极值
                            q1 = values.quantile(0.25)
                            q3 = values.quantile(0.75)
                            iqr = q3 - q1
                            lower_bound = q1 - 1.5 * iqr
                            upper_bound = q3 + 1.5 * iqr

                            outliers = values[(values < lower_bound) | (values > upper_bound)]
                            outlier_rate = len(outliers) / len(values)

                            if outlier_rate > 0.1:  # 超过10%的异常值
                                consistency_report['consistency_score'] *= 0.7
                                consistency_report['inconsistencies'].append(
                                    f"{field} 异常值比例过高: {outlier_rate:.2%}"
                                )

            return consistency_report

        except Exception as e:
            self.logger.error(f"检查数据一致性失败: {e}")
            return {'consistency_score': 0.0, 'error': str(e)}

    def _calculate_validation_score(self, integrity_result: Dict, anomaly_result: List, consistency_result: Dict) -> float:
        """计算验证评分"""
        try:
            # 权重配置
            weights = {
                'integrity': 0.4,
                'anomaly': 0.4,
                'consistency': 0.2
            }

            # 计算各项得分
            integrity_score = integrity_result.get('integrity_score', 0.0)

            anomaly_score = 1.0
            if anomaly_result:
                # 根据异常严重程度计算扣分
                severity_weights = {'critical': 0.3, 'high': 0.2, 'medium': 0.1, 'low': 0.05}
                total_deduction = 0

                for anomaly in anomaly_result:
                    severity = anomaly.get('severity', 'low')
                    confidence = anomaly.get('confidence', 1.0)
                    deduction = severity_weights.get(severity, 0.05) * confidence
                    total_deduction += deduction

                anomaly_score = max(0.0, 1.0 - total_deduction)

            consistency_score = consistency_result.get('consistency_score', 0.0)

            # 计算综合得分
            total_score = (
                integrity_score * weights['integrity'] +
                anomaly_score * weights['anomaly'] +
                consistency_score * weights['consistency']
            )

            return total_score

        except Exception as e:
            self.logger.error(f"计算验证评分失败: {e}")
            return 0.0

    async def _generate_recommendations(self, integrity_result: Dict, anomaly_result: List, consistency_result: Dict) -> List[str]:
        """生成修复建议"""
        recommendations = []

        try:
            # 基于完整性问题的建议
            if integrity_result.get('missing_records', 0) > 0:
                recommendations.append("检测到缺失数据，建议启用数据插值修复")

            if integrity_result.get('duplicate_records', 0) > 0:
                recommendations.append("检测到重复数据，建议进行数据去重处理")

            if integrity_result.get('timeline_gaps', 0) > 0:
                recommendations.append("检测到时间线断裂，建议检查数据源连接")

            # 基于异常的建议
            if anomaly_result:
                high_severity_anomalies = [a for a in anomaly_result if a.get('severity') in ['high', 'critical']]
                if high_severity_anomalies:
                    recommendations.append("检测到高严重度异常，建议立即检查数据源")

                price_anomalies = [a for a in anomaly_result if a.get('type') == 'price_anomaly']
                if price_anomalies:
                    recommendations.append("检测到价格异常，建议验证市场数据准确性")

                volume_anomalies = [a for a in anomaly_result if a.get('type') == 'volume_spike']
                if volume_anomalies:
                    recommendations.append("检测到成交量异常，建议确认市场活跃度")

            # 基于一致性问题的建议
            inconsistencies = consistency_result.get('inconsistencies', [])
            if inconsistencies:
                recommendations.append("检测到数据一致性问题，建议进行数据标准化")

        except Exception as e:
            self.logger.error(f"生成修复建议失败: {e}")

        return recommendations

    async def _update_validation_stats(self, record_count: int, anomaly_count: int, validation_score: float):
        """更新验证统计信息"""
        try:
            self.validation_stats['total_validated'] += record_count
            self.validation_stats['total_anomalies'] += anomaly_count

            # 更新平均验证分数
            total_validated = self.validation_stats['total_validated']
            if total_validated > 0:
                current_avg = self.validation_stats['avg_validation_score']
                new_score = validation_score
                self.validation_stats['avg_validation_score'] = \
                    (current_avg * (total_validated - record_count) + new_score * record_count) / total_validated

            # 更新异常率
            if total_validated > 0:
                self.validation_stats['anomaly_rate'] = \
                    self.validation_stats['total_anomalies'] / total_validated

        except Exception as e:
            self.logger.error(f"更新验证统计信息失败: {e}")

    async def _trigger_validation_alert(self, exchange: str, symbol: str, validation_score: float, anomalies: List):
        """触发验证告警"""
        try:
            # 检查告警条件
            should_alert = False
            alert_severity = AlertSeverity.LOW
            alert_message = ""

            # 验证分数过低
            if validation_score < 0.5:
                should_alert = True
                alert_severity = AlertSeverity.CRITICAL
                alert_message = f"数据验证分数过低: {validation_score:.2f}"
            elif validation_score < 0.8:
                should_alert = True
                alert_severity = AlertSeverity.HIGH
                alert_message = f"数据验证分数异常: {validation_score:.2f}"

            # 检查异常数量
            if len(anomalies) > 10:
                should_alert = True
                alert_severity = AlertSeverity.HIGH
                alert_message = f"检测到大量异常: {len(anomalies)} 个"

            # 检查高严重度异常
            critical_anomalies = [a for a in anomalies if a.get('severity') == 'critical']
            if critical_anomalies:
                should_alert = True
                alert_severity = AlertSeverity.CRITICAL
                alert_message = f"检测到严重异常: {len(critical_anomalies)} 个"

            if should_alert:
                # 创建告警记录
                alert_record = {
                    'exchange': exchange,
                    'symbol': symbol,
                    'severity': alert_severity.value,
                    'message': alert_message,
                    'validation_score': validation_score,
                    'anomaly_count': len(anomalies),
                    'timestamp': datetime.now().isoformat()
                }

                # 保存告警
                await self._save_validation_alert(alert_record)

                # 记录指标
                metrics.record_validation_alert(
                    exchange=exchange,
                    symbol=symbol,
                    severity=alert_severity.value,
                    validation_score=validation_score
                )

                self.logger.warning(f"数据验证告警: {alert_message}")

        except Exception as e:
            self.logger.error(f"触发验证告警失败: {e}")

    async def _save_validation_alert(self, alert_record: Dict):
        """保存验证告警"""
        try:
            # 保存到数据库
            async with get_db_session() as session:
                # 这里应该保存到AnomalyRecord表
                # 简化实现，记录到Redis
                alert_key = f"validation_alert_{int(time.time())}"
                await redis_client.setex(alert_key, 86400, json.dumps(alert_record))  # 24小时过期

        except Exception as e:
            self.logger.error(f"保存验证告警失败: {e}")

    async def repair_data(self, df: pd.DataFrame, anomalies: List[Dict], repair_methods: List[RepairAction] = None) -> RepairResult:
        """修复数据异常"""
        try:
            if repair_methods is None:
                repair_methods = [
                    RepairAction.INTERPOLATE,
                    RepairAction.FILL_FORWARD,
                    RepairAction.MEDIAN_IMPUTATION
                ]

            repaired_df = df.copy()
            repair_stats = {
                RepairAction.INTERPOLATE.value: 0,
                RepairAction.FILL_FORWARD.value: 0,
                RepairAction.FILL_BACKWARD.value: 0,
                RepairAction.MEAN_IMPUTATION.value: 0,
                RepairAction.MEDIAN_IMPUTATION.value: 0,
                RepairAction.REMOVE_RECORD.value: 0,
                RepairAction.FLAG_RECORD.value: 0
            }

            failed_repairs = 0

            for anomaly in anomalies:
                try:
                    record_index = anomaly.get('record_index')
                    field = anomaly.get('field')
                    anomaly_type = anomaly.get('type')

                    if record_index is None or field is None:
                        continue

                    success = False
                    for method in repair_methods:
                        if success:
                            break

                        if method == RepairAction.INTERPOLATE:
                            success = await self._repair_interpolate(repaired_df, record_index, field)
                        elif method == RepairAction.FILL_FORWARD:
                            success = await self._repair_fill_forward(repaired_df, record_index, field)
                        elif method == RepairAction.MEDIAN_IMPUTATION:
                            success = await self._repair_median_imputation(repaired_df, record_index, field)
                        elif method == RepairAction.REMOVE_RECORD:
                            success = await self._repair_remove_record(repaired_df, record_index)

                    if success:
                        repair_stats[method.value] += 1
                    else:
                        failed_repairs += 1

                except Exception as e:
                    self.logger.error(f"修复异常失败: {e}")
                    failed_repairs += 1

            total_repairs = sum(repair_stats.values())
            return RepairResult(
                success=failed_repairs == 0,
                repaired_count=total_repairs,
                failed_repairs=failed_repairs,
                repair_methods=repair_stats,
                repair_stats={
                    'total_anomalies': len(anomalies),
                    'repair_success_rate': total_repairs / max(len(anomalies), 1),
                    'records_processed': len(repaired_df)
                }
            )

        except Exception as e:
            self.logger.error(f"修复数据失败: {e}")
            return RepairResult(
                success=False,
                repaired_count=0,
                failed_repairs=len(anomalies),
                repair_methods={},
                repair_stats={}
            )

    async def _repair_interpolate(self, df: pd.DataFrame, index: int, field: str) -> bool:
        """插值修复"""
        try:
            if field not in df.columns:
                return False

            # 线性插值
            if index > 0 and index < len(df) - 1:
                prev_value = df.iloc[index - 1][field]
                next_value = df.iloc[index + 1][field]

                if pd.notna(prev_value) and pd.notna(next_value):
                    interpolated_value = (prev_value + next_value) / 2
                    df.at[index, field] = interpolated_value
                    return True

            return False

        except Exception as e:
            self.logger.error(f"插值修复失败: {e}")
            return False

    async def _repair_fill_forward(self, df: pd.DataFrame, index: int, field: str) -> bool:
        """向前填充修复"""
        try:
            if field not in df.columns:
                return False

            if index > 0:
                prev_value = df.iloc[index - 1][field]
                if pd.notna(prev_value):
                    df.at[index, field] = prev_value
                    return True

            return False

        except Exception as e:
            self.logger.error(f"向前填充修复失败: {e}")
            return False

    async def _repair_median_imputation(self, df: pd.DataFrame, index: int, field: str) -> bool:
        """中位数填充修复"""
        try:
            if field not in df.columns:
                return False

            # 使用中位数填充
            median_value = df[field].median()
            if pd.notna(median_value):
                df.at[index, field] = median_value
                return True

            return False

        except Exception as e:
            self.logger.error(f"中位数填充修复失败: {e}")
            return False

    async def _repair_remove_record(self, df: pd.DataFrame, index: int) -> bool:
        """删除记录修复"""
        try:
            df.drop(index, inplace=True)
            return True

        except Exception as e:
            self.logger.error(f"删除记录修复失败: {e}")
            return False

    async def _validation_monitor_loop(self):
        """验证监控循环"""
        while self._running:
            try:
                # 检查验证统计
                await self._check_validation_stats()

                # 清理过期告警
                await self._clean_expired_alerts()

                # 更新异常检测模型
                if int(time.time()) % 3600 == 0:  # 每小时更新一次
                    await self._update_anomaly_model()

                await asyncio.sleep(60)  # 每分钟检查一次

            except Exception as e:
                self.logger.error(f"验证监控失败: {e}")
                await asyncio.sleep(10)

    async def _check_validation_stats(self):
        """检查验证统计"""
        try:
            stats = self.validation_stats

            # 检查异常率
            if stats['anomaly_rate'] > self.alert_thresholds['anomaly_rate']:
                self.logger.warning(f"数据异常率过高: {stats['anomaly_rate']:.2%}")

            # 检查平均验证分数
            if stats['avg_validation_score'] < 0.8:
                self.logger.warning(f"平均验证分数过低: {stats['avg_validation_score']:.2f}")

        except Exception as e:
            self.logger.error(f"检查验证统计失败: {e}")

    async def _clean_expired_alerts(self):
        """清理过期告警"""
        try:
            # 清理超过24小时的告警
            expired_keys = []
            for key in await redis_client.keys("validation_alert_*"):
                ttl = await redis_client.ttl(key)
                if ttl > 86400:  # 超过24小时
                    expired_keys.append(key)

            if expired_keys:
                await redis_client.delete(*expired_keys)
                self.logger.info(f"清理了 {len(expired_keys)} 个过期告警")

        except Exception as e:
            self.logger.error(f"清理过期告警失败: {e}")

    async def _update_anomaly_model(self):
        """更新异常检测模型"""
        try:
            # 重新加载历史数据
            await self._load_historical_data()

            # 重新训练模型
            await self._train_anomaly_detection_model()

            self.logger.info("异常检测模型更新完成")

        except Exception as e:
            self.logger.error(f"更新异常检测模型失败: {e}")

    async def get_validation_report(self, exchange: str = None, symbol: str = None, hours: int = 24) -> Dict[str, Any]:
        """获取验证报告"""
        try:
            report = {
                'report_period': f"{hours} hours",
                'generated_at': datetime.now().isoformat(),
                'exchange': exchange,
                'symbol': symbol,
                'validation_stats': self.validation_stats.copy(),
                'anomaly_summary': {
                    'total_anomalies': self.validation_stats['total_anomalies'],
                    'anomaly_rate': self.validation_stats['anomaly_rate'],
                    'avg_validation_score': self.validation_stats['avg_validation_score']
                },
                'recommendations': await self._get_validation_recommendations()
            }

            return report

        except Exception as e:
            self.logger.error(f"获取验证报告失败: {e}")
            return {}

    async def _get_validation_recommendations(self) -> List[str]:
        """获取验证建议"""
        recommendations = []

        try:
            stats = self.validation_stats

            # 基于异常率的建议
            if stats['anomaly_rate'] > self.alert_thresholds['anomaly_rate']:
                recommendations.append(f"异常率过高({stats['anomaly_rate']:.2%})，建议检查数据源质量")

            # 基于验证分数的建议
            if stats['avg_validation_score'] < 0.8:
                recommendations.append(f"平均验证分数过低({stats['avg_validation_score']:.2f})，建议优化验证规则")

            # 基于处理量的建议
            if stats['total_validated'] == 0:
                recommendations.append("暂无验证数据，请检查数据收集状态")

        except Exception as e:
            self.logger.error(f"获取验证建议失败: {e}")

        return recommendations

    async def close(self):
        """关闭数据验证器"""
        self._running = False

        if self._monitoring_task:
            self._monitoring_task.cancel()

        self.logger.info("Data Validator 已关闭")

# 全局数据验证器实例
data_validator = DataValidator()
```

## 接受标准

### 必须满足的条件
- [ ] 实现完整的数据验证系统，包括完整性、一致性和准确性检查
- [ ] 支持多种异常检测方法（统计、机器学习、规则基础）
- [ ] 提供自动修复机制和多种修复策略
- [ ] 实现实时异常告警和通知系统
- [ ] 支持验证报告生成和分析
- [ ] 实现高性能的验证处理架构
- [ ] 提供详细的异常记录和追踪
- [ ] 支持配置热更新和规则动态调整
- [ ] 通过性能测试和准确性验证

### 性能要求
- **验证延迟**: 单批数据验证延迟 < 50ms
- **异常检测准确率**: 异常检测准确率 > 85%
- **修复成功率**: 自动修复成功率 > 80%
- **告警响应时间**: 异常告警响应时间 < 10秒
- **并发处理**: 支持1000+并发验证请求

### 技术规范
- 使用多种异常检测算法
- 实现异步处理架构
- 支持多线程并发处理
- 完整的错误处理和异常管理
- 符合数据安全和隐私要求

## 实现步骤

### 第一阶段：基础验证 (2天)
1. 设计数据验证核心架构
2. 实现基础完整性检查
3. 开发一致性验证功能
4. 实现基础异常检测

### 第二阶段：异常检测 (3天)
1. 实现统计异常检测算法
2. 开发机器学习异常检测
3. 实现规则基础异常检测
4. 添加异常评分机制

### 第三阶段：自动修复 (2天)
1. 实现多种修复策略
2. 开发智能修复算法
3. 添加修复验证机制
4. 实现修复结果统计

### 第四阶段：告警和报告 (1天)
1. 实现异常告警系统
2. 开发验证报告生成
3. 添加监控和统计功能
4. 完善配置管理

## 交付物

### 文档
- 数据验证架构设计文档
- 异常检测算法文档
- 修复机制文档
- 告警系统文档

### 代码
- 数据验证器核心实现
- 异常检测模块
- 自动修复模块
- 告警系统
- 报告生成器
- 单元测试和集成测试

### 配置
- 验证规则配置文件
- 异常检测参数配置
- 告警阈值配置
- 修复策略配置

## 风险和依赖

### 技术风险
- 异常检测算法可能存在误报和漏报
- 自动修复可能引入新的数据问题
- 机器学习模型训练需要足够的历史数据

### 依赖关系
- 依赖于数据处理器提供的清洗后数据
- 依赖于数据库性能和可用性
- 依赖于缓存系统的稳定性

### 缓解措施
- 实现多种检测方法的交叉验证
- 添加修复后的验证机制
- 提供手动干预和回滚功能

## 验收标准
- 数据验证准确可靠
- 异常检测有效及时
- 自动修复成功率达标
- 告警系统响应及时
- 报告生成完整准确
- 系统性能满足要求