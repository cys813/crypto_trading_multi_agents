---
name: Data Processor - 数据处理和质量控制
type: task
epic: 数据收集代理
status: open
priority: 2
created: 2025-09-25T20:20:35Z
estimated: 10 days
assigned: [待分配]
parallelizable: true
dependencies: ["005", "006"]
---

# 任务: Data Processor - 数据处理和质量控制

## 任务描述
实现完整的数据处理和质量控制机制，包括数据清洗、验证、转换和质量控制。通过自动化的数据处理管道，确保所有收集的数据都符合高质量标准，为整个交易系统提供可靠、一致的数据基础。

## 技术要求

### 核心架构设计

#### Data Processor 组件
- **数据清洗**: 自动检测和修复数据格式、类型、范围异常
- **数据验证**: 实施完整性和准确性检查，确保数据一致性
- **数据转换**: 统一不同交易所的数据格式和结构
- **质量控制**: 实时数据质量评分和监控
- **自动修复**: 智能修复常见数据问题

#### 技术栈
- **核心库**: Python 3.8+ + AsyncIO
- **数据处理**: Pandas + NumPy + Polars (高性能数据处理)
- **验证**: Pydantic + Cerberus (数据验证)
- **缓存**: Redis (数据缓存和状态管理)
- **监控**: Prometheus指标收集

### 实现架构

```python
# src/processors/data_processor.py
import asyncio
import time
from typing import Dict, List, Optional, Any, Callable, Union
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from decimal import Decimal
from enum import Enum
import numpy as np
import pandas as pd
import polars as pl
import logging
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_, or_
import json
from concurrent.futures import ThreadPoolExecutor
import re

from ..core.config import config
from ..core.logger import get_logger
from ..core.metrics import metrics
from ..core.exceptions import DataProcessingError, ValidationError
from ..managers.exchange_manager import exchange_manager
from ..storage.postgresql import get_db_session
from ..storage.redis import redis_client
from ..models.market_data import OHLCV, Ticker, OrderBook, Trade
from ..models.quality import DataQuality, QualityMetric, ProcessingLog

class DataStatus(Enum):
    """数据状态枚举"""
    RAW = "raw"
    CLEANED = "cleaned"
    VALIDATED = "validated"
    ENRICHED = "enriched"
    STORED = "stored"

class QualityLevel(Enum):
    """质量等级枚举"""
    EXCELLENT = "excellent"  # 95-100%
    GOOD = "good"           # 85-94%
    FAIR = "fair"           # 70-84%
    POOR = "poor"           # 50-69%
    CRITICAL = "critical"    # <50%

@dataclass
class ValidationRule:
    """验证规则"""
    name: str
    field: str
    rule_type: str  # range, format, regex, custom
    parameters: Dict[str, Any]
    severity: str  # info, warning, error, critical
    action: str    # log, repair, reject, alert

@dataclass
class ProcessingResult:
    """处理结果"""
    success: bool
    processed_count: int
    error_count: int
    repaired_count: int
    rejected_count: int
    quality_score: float
    processing_time: float
    errors: List[str]
    warnings: List[str]

class DataProcessor:
    """数据处理器"""

    def __init__(self):
        self.logger = get_logger("data_processor")
        self.batch_size = 1000
        self.max_workers = 4
        self.cache_ttl = 300  # 5分钟缓存
        self.quality_thresholds = {
            'min_quality_score': 0.7,
            'max_error_rate': 0.05,
            'max_repair_rate': 0.1
        }
        self._running = False
        self._processing_tasks = []
        self._executor = ThreadPoolExecutor(max_workers=self.max_workers)

        # 验证规则配置
        self.validation_rules = self._load_validation_rules()

        # 处理统计
        self.processing_stats = {
            'total_processed': 0,
            'total_errors': 0,
            'total_repaired': 0,
            'total_rejected': 0,
            'avg_quality_score': 0.0,
            'avg_processing_time': 0.0
        }

    def _load_validation_rules(self) -> Dict[str, List[ValidationRule]]:
        """加载验证规则"""
        return {
            'ohlcv': [
                ValidationRule(
                    name="price_range_check",
                    field="close",
                    rule_type="range",
                    parameters={"min": 0, "max": 1000000},
                    severity="error",
                    action="reject"
                ),
                ValidationRule(
                    name="volume_range_check",
                    field="volume",
                    rule_type="range",
                    parameters={"min": 0},
                    severity="warning",
                    action="repair"
                ),
                ValidationRule(
                    name="timestamp_format",
                    field="timestamp",
                    rule_type="format",
                    parameters={"format": "timestamp"},
                    severity="error",
                    action="repair"
                ),
                ValidationRule(
                    name="ohlc_consistency",
                    field="ohlc",
                    rule_type="custom",
                    parameters={"check_order": True},
                    severity="error",
                    action="repair"
                )
            ],
            'ticker': [
                ValidationRule(
                    name="price_positive",
                    field="last",
                    rule_type="range",
                    parameters={"min": 0},
                    severity="error",
                    action="reject"
                ),
                ValidationRule(
                    name="volume_non_negative",
                    field="volume",
                    rule_type="range",
                    parameters={"min": 0},
                    severity="warning",
                    action="repair"
                )
            ],
            'orderbook': [
                ValidationRule(
                    name="price_levels_valid",
                    field="price",
                    rule_type="range",
                    parameters={"min": 0},
                    severity="error",
                    action="repair"
                ),
                ValidationRule(
                    name="quantity_non_negative",
                    field="quantity",
                    rule_type="range",
                    parameters={"min": 0},
                    severity="warning",
                    action="repair"
                ),
                ValidationRule(
                    name="orderbook_depth",
                    field="levels",
                    rule_type="range",
                    parameters={"min": 1, "max": 1000},
                    severity="warning",
                    action="log"
                )
            ],
            'trade': [
                ValidationRule(
                    name="trade_price_positive",
                    field="price",
                    rule_type="range",
                    parameters={"min": 0},
                    severity="error",
                    action="reject"
                ),
                ValidationRule(
                    name="trade_quantity_positive",
                    field="quantity",
                    rule_type="range",
                    parameters={"min": 0},
                    severity="error",
                    action="reject"
                ),
                ValidationRule(
                    name="trade_id_unique",
                    field="trade_id",
                    rule_type="custom",
                    parameters={"check_uniqueness": True},
                    severity="warning",
                    action="log"
                )
            ]
        }

    async def initialize(self):
        """初始化数据处理器"""
        try:
            # 检查Redis连接
            await redis_client.ping()

            # 启动处理监控任务
            self._running = True
            asyncio.create_task(self._processing_monitor_loop())

            # 加载历史处理规则
            await self._load_processing_rules()

            self.logger.info("Data Processor 初始化完成")

        except Exception as e:
            self.logger.error(f"Data Processor 初始化失败: {e}")
            raise

    async def _load_processing_rules(self):
        """加载处理规则"""
        try:
            # 从Redis加载自定义处理规则
            custom_rules = await redis_client.get("processing_rules")
            if custom_rules:
                rules_data = json.loads(custom_rules)
                # 合并自定义规则到默认规则
                for data_type, rules in rules_data.items():
                    if data_type in self.validation_rules:
                        self.validation_rules[data_type].extend(rules)
                    else:
                        self.validation_rules[data_type] = rules

        except Exception as e:
            self.logger.error(f"加载处理规则失败: {e}")

    async def process_ohlcv_data(self, exchange: str, symbol: str, data: List[Dict]) -> ProcessingResult:
        """处理OHLCV数据"""
        try:
            start_time = time.time()

            # 转换为DataFrame进行处理
            df = pd.DataFrame(data)

            # 数据清洗
            cleaned_df = await self._clean_ohlcv_data(df, exchange, symbol)

            # 数据验证
            validated_df, validation_result = await self._validate_ohlcv_data(cleaned_df)

            # 数据转换
            transformed_df = await self._transform_ohlcv_data(validated_df, exchange, symbol)

            # 质量评分
            quality_score = await self._calculate_quality_score(transformed_df, 'ohlcv')

            # 数据增强
            enriched_df = await self._enrich_ohlcv_data(transformed_df, exchange, symbol)

            # 存储处理后的数据
            await self._store_processed_data(enriched_df, 'ohlcv', exchange, symbol)

            # 记录处理结果
            processing_time = time.time() - start_time
            result = ProcessingResult(
                success=True,
                processed_count=len(enriched_df),
                error_count=validation_result['error_count'],
                repaired_count=validation_result['repaired_count'],
                rejected_count=validation_result['rejected_count'],
                quality_score=quality_score,
                processing_time=processing_time,
                errors=validation_result['errors'],
                warnings=validation_result['warnings']
            )

            # 更新统计信息
            self._update_processing_stats(result)

            # 记录指标
            metrics.record_data_processing(
                data_type='ohlcv',
                processed_count=result.processed_count,
                quality_score=quality_score,
                processing_time=processing_time
            )

            return result

        except Exception as e:
            self.logger.error(f"处理OHLCV数据失败: {e}")
            return ProcessingResult(
                success=False,
                processed_count=0,
                error_count=len(data),
                repaired_count=0,
                rejected_count=len(data),
                quality_score=0.0,
                processing_time=0.0,
                errors=[str(e)],
                warnings=[]
            )

    async def _clean_ohlcv_data(self, df: pd.DataFrame, exchange: str, symbol: str) -> pd.DataFrame:
        """清洗OHLCV数据"""
        try:
            cleaned_df = df.copy()

            # 处理缺失值
            cleaned_df = cleaned_df.dropna(subset=['timestamp', 'open', 'high', 'low', 'close'])

            # 数据类型转换
            cleaned_df['timestamp'] = pd.to_datetime(cleaned_df['timestamp'], unit='ms')

            # 价格字段转换
            for field in ['open', 'high', 'low', 'close']:
                cleaned_df[field] = pd.to_numeric(cleaned_df[field], errors='coerce')

            # 成交量转换
            cleaned_df['volume'] = pd.to_numeric(cleaned_df['volume'], errors='coerce')

            # 删除无效价格数据
            cleaned_df = cleaned_df[
                (cleaned_df['open'] > 0) &
                (cleaned_df['high'] > 0) &
                (cleaned_df['low'] > 0) &
                (cleaned_df['close'] > 0)
            ]

            # 时间戳去重
            cleaned_df = cleaned_df.drop_duplicates(subset=['timestamp'])

            # 按时间排序
            cleaned_df = cleaned_df.sort_values('timestamp')

            # 记录清洗统计
            original_count = len(df)
            cleaned_count = len(cleaned_df)

            if original_count != cleaned_count:
                self.logger.info(f"OHLCV数据清洗: {exchange}/{symbol}, 原始记录数: {original_count}, 清洗后: {cleaned_count}")

            return cleaned_df

        except Exception as e:
            self.logger.error(f"清洗OHLCV数据失败: {e}")
            raise DataProcessingError(f"数据清洗失败: {e}")

    async def _validate_ohlcv_data(self, df: pd.DataFrame) -> tuple[pd.DataFrame, Dict[str, Any]]:
        """验证OHLCV数据"""
        try:
            validated_df = df.copy()
            validation_result = {
                'error_count': 0,
                'repaired_count': 0,
                'rejected_count': 0,
                'errors': [],
                'warnings': []
            }

            # 获取验证规则
            rules = self.validation_rules.get('ohlcv', [])

            for rule in rules:
                if rule.field == 'ohlc' and rule.rule_type == 'custom':
                    # OHLC一致性检查
                    result = await self._validate_ohlc_consistency(validated_df, rule)
                    validation_result['errors'].extend(result['errors'])
                    validation_result['warnings'].extend(result['warnings'])
                    validation_result['repaired_count'] += result['repaired_count']

                elif rule.rule_type == 'range':
                    result = await self._validate_range(validated_df, rule)
                    validation_result['errors'].extend(result['errors'])
                    validation_result['warnings'].extend(result['warnings'])
                    validation_result['repaired_count'] += result['repaired_count']
                    validation_result['rejected_count'] += result['rejected_count']

                elif rule.rule_type == 'format':
                    result = await self._validate_format(validated_df, rule)
                    validation_result['errors'].extend(result['errors'])
                    validation_result['warnings'].extend(result['warnings'])
                    validation_result['repaired_count'] += result['repaired_count']

            return validated_df, validation_result

        except Exception as e:
            self.logger.error(f"验证OHLCV数据失败: {e}")
            raise ValidationError(f"数据验证失败: {e}")

    async def _validate_ohlc_consistency(self, df: pd.DataFrame, rule: ValidationRule) -> Dict[str, Any]:
        """验证OHLC一致性"""
        result = {'errors': [], 'warnings': [], 'repaired_count': 0, 'rejected_count': 0}

        try:
            # 检查high >= low
            invalid_hl = df[df['high'] < df['low']]
            if not invalid_hl.empty:
                result['errors'].append(f"发现 {len(invalid_hl)} 条记录的high < low")

                # 自动修复：交换high和low
                df.loc[invalid_hl.index, 'high'], df.loc[invalid_hl.index, 'low'] = \
                    df.loc[invalid_hl.index, 'low'], df.loc[invalid_hl.index, 'high']
                result['repaired_count'] += len(invalid_hl)

            # 检查high >= open, high >= close
            invalid_high = df[(df['high'] < df['open']) | (df['high'] < df['close'])]
            if not invalid_high.empty:
                result['warnings'].append(f"发现 {len(invalid_high)} 条记录的high不是最高价")

                # 修复：设置high为max(open, close, high)
                df.loc[invalid_high.index, 'high'] = df.loc[invalid_high.index, ['open', 'close', 'high']].max(axis=1)
                result['repaired_count'] += len(invalid_high)

            # 检查low <= open, low <= close
            invalid_low = df[(df['low'] > df['open']) | (df['low'] > df['close'])]
            if not invalid_low.empty:
                result['warnings'].append(f"发现 {len(invalid_low)} 条记录的low不是最低价")

                # 修复：设置low为min(open, close, low)
                df.loc[invalid_low.index, 'low'] = df.loc[invalid_low.index, ['open', 'close', 'low']].min(axis=1)
                result['repaired_count'] += len(invalid_low)

        except Exception as e:
            result['errors'].append(f"OHLC一致性验证失败: {e}")

        return result

    async def _validate_range(self, df: pd.DataFrame, rule: ValidationRule) -> Dict[str, Any]:
        """验证数值范围"""
        result = {'errors': [], 'warnings': [], 'repaired_count': 0, 'rejected_count': 0}

        try:
            field = rule.field
            min_val = rule.parameters.get('min', None)
            max_val = rule.parameters.get('max', None)

            if field not in df.columns:
                result['errors'].append(f"字段 {field} 不存在")
                return result

            # 检查最小值
            if min_val is not None:
                invalid_min = df[df[field] < min_val]
                if not invalid_min.empty:
                    message = f"发现 {len(invalid_min)} 条记录的{field} < {min_val}"

                    if rule.severity == 'error':
                        result['errors'].append(message)
                    else:
                        result['warnings'].append(message)

                    if rule.action == 'repair':
                        # 修复为最小值
                        df.loc[invalid_min.index, field] = min_val
                        result['repaired_count'] += len(invalid_min)
                    elif rule.action == 'reject':
                        # 删除无效记录
                        df.drop(invalid_min.index, inplace=True)
                        result['rejected_count'] += len(invalid_min)

            # 检查最大值
            if max_val is not None:
                invalid_max = df[df[field] > max_val]
                if not invalid_max.empty:
                    message = f"发现 {len(invalid_max)} 条记录的{field} > {max_val}"

                    if rule.severity == 'error':
                        result['errors'].append(message)
                    else:
                        result['warnings'].append(message)

                    if rule.action == 'repair':
                        # 修复为最大值
                        df.loc[invalid_max.index, field] = max_val
                        result['repaired_count'] += len(invalid_max)
                    elif rule.action == 'reject':
                        # 删除无效记录
                        df.drop(invalid_max.index, inplace=True)
                        result['rejected_count'] += len(invalid_max)

        except Exception as e:
            result['errors'].append(f"范围验证失败: {e}")

        return result

    async def _validate_format(self, df: pd.DataFrame, rule: ValidationRule) -> Dict[str, Any]:
        """验证数据格式"""
        result = {'errors': [], 'warnings': [], 'repaired_count': 0, 'rejected_count': 0}

        try:
            field = rule.field
            format_type = rule.parameters.get('format', '')

            if field not in df.columns:
                result['errors'].append(f"字段 {field} 不存在")
                return result

            if format_type == 'timestamp':
                # 验证时间戳格式
                invalid_format = df[~df[field].astype(str).str.match(r'^\d{13}$')]
                if not invalid_format.empty:
                    result['errors'].append(f"发现 {len(invalid_format)} 条记录的时间戳格式无效")

                    # 尝试修复
                    if rule.action == 'repair':
                        # 尝试将ISO格式转换为时间戳
                        for idx in invalid_format.index:
                            try:
                                timestamp = pd.to_datetime(df.loc[idx, field])
                                df.loc[idx, field] = int(timestamp.timestamp() * 1000)
                                result['repaired_count'] += 1
                            except:
                                # 如果修复失败，删除记录
                                df.drop(idx, inplace=True)
                                result['rejected_count'] += 1
                    elif rule.action == 'reject':
                        df.drop(invalid_format.index, inplace=True)
                        result['rejected_count'] += len(invalid_format)

        except Exception as e:
            result['errors'].append(f"格式验证失败: {e}")

        return result

    async def _transform_ohlcv_data(self, df: pd.DataFrame, exchange: str, symbol: str) -> pd.DataFrame:
        """转换OHLCV数据"""
        try:
            transformed_df = df.copy()

            # 添加交易所标识
            transformed_df['exchange'] = exchange

            # 添加交易对
            transformed_df['symbol'] = symbol

            # 添加数据质量标识
            transformed_df['quality_score'] = 1.0
            transformed_df['status'] = DataStatus.VALIDATED.value

            # 添加处理时间戳
            transformed_df['processed_at'] = datetime.now()

            # 计算价格变化
            transformed_df['price_change'] = transformed_df['close'] - transformed_df['open']
            transformed_df['price_change_pct'] = (transformed_df['price_change'] / transformed_df['open']) * 100

            # 计算交易量加权平均价格
            transformed_df['vwap'] = (transformed_df['volume'] * (transformed_df['high'] + transformed_df['low'] + transformed_df['close']) / 3) / transformed_df['volume']

            # 添加时间特征
            transformed_df['hour'] = transformed_df['timestamp'].dt.hour
            transformed_df['day_of_week'] = transformed_df['timestamp'].dt.dayofweek
            transformed_df['day_of_month'] = transformed_df['timestamp'].dt.day

            return transformed_df

        except Exception as e:
            self.logger.error(f"转换OHLCV数据失败: {e}")
            raise DataProcessingError(f"数据转换失败: {e}")

    async def _enrich_ohlcv_data(self, df: pd.DataFrame, exchange: str, symbol: str) -> pd.DataFrame:
        """增强OHLCV数据"""
        try:
            enriched_df = df.copy()

            # 添加技术指标（如果数据量足够）
            if len(enriched_df) >= 20:
                # 计算移动平均
                enriched_df['ma_5'] = enriched_df['close'].rolling(window=5).mean()
                enriched_df['ma_10'] = enriched_df['close'].rolling(window=10).mean()
                enriched_df['ma_20'] = enriched_df['close'].rolling(window=20).mean()

                # 计算RSI
                enriched_df['rsi_14'] = await self._calculate_rsi(enriched_df['close'], 14)

                # 计算布林带
                enriched_df['bb_upper'], enriched_df['bb_lower'] = await self._calculate_bollinger_bands(enriched_df['close'], 20)

            # 添加市场状态标记
            enriched_df['market_status'] = enriched_df.apply(
                lambda row: self._classify_market_status(row), axis=1
            )

            # 添加数据质量标记
            enriched_df['quality_level'] = enriched_df.apply(
                lambda row: self._classify_quality_level(row), axis=1
            )

            return enriched_df

        except Exception as e:
            self.logger.error(f"增强OHLCV数据失败: {e}")
            # 返回未增强的数据，不抛出异常
            return df

    async def _calculate_rsi(self, prices: pd.Series, period: int) -> pd.Series:
        """计算RSI指标"""
        try:
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi
        except Exception as e:
            self.logger.error(f"计算RSI失败: {e}")
            return pd.Series(index=prices.index, dtype=float)

    async def _calculate_bollinger_bands(self, prices: pd.Series, period: int, std_dev: int = 2) -> tuple[pd.Series, pd.Series]:
        """计算布林带"""
        try:
            ma = prices.rolling(window=period).mean()
            std = prices.rolling(window=period).std()
            upper = ma + (std * std_dev)
            lower = ma - (std * std_dev)
            return upper, lower
        except Exception as e:
            self.logger.error(f"计算布林带失败: {e}")
            return pd.Series(index=prices.index, dtype=float), pd.Series(index=prices.index, dtype=float)

    def _classify_market_status(self, row: pd.Series) -> str:
        """分类市场状态"""
        try:
            price_change_pct = row.get('price_change_pct', 0)
            volume = row.get('volume', 0)

            if abs(price_change_pct) < 0.1:
                return 'flat'
            elif price_change_pct > 2:
                return 'bullish'
            elif price_change_pct < -2:
                return 'bearish'
            elif price_change_pct > 0:
                return 'up'
            else:
                return 'down'

        except Exception:
            return 'unknown'

    def _classify_quality_level(self, row: pd.Series) -> str:
        """分类质量等级"""
        try:
            quality_score = row.get('quality_score', 1.0)

            if quality_score >= 0.95:
                return QualityLevel.EXCELLENT.value
            elif quality_score >= 0.85:
                return QualityLevel.GOOD.value
            elif quality_score >= 0.70:
                return QualityLevel.FAIR.value
            elif quality_score >= 0.50:
                return QualityLevel.POOR.value
            else:
                return QualityLevel.CRITICAL.value

        except Exception:
            return QualityLevel.GOOD.value

    async def _calculate_quality_score(self, df: pd.DataFrame, data_type: str) -> float:
        """计算数据质量评分"""
        try:
            if df.empty:
                return 0.0

            # 基础质量指标
            completeness_score = len(df.dropna()) / len(df)

            # 数据一致性评分
            consistency_score = 1.0
            if data_type == 'ohlcv':
                # OHLC一致性
                ohlc_consistent = ((df['high'] >= df['low']) &
                                 (df['high'] >= df['open']) &
                                 (df['high'] >= df['close']) &
                                 (df['low'] <= df['open']) &
                                 (df['low'] <= df['close'])).mean()
                consistency_score = ohlc_consistent

            # 数据时效性评分（基于最新数据的时间差）
            recency_score = 1.0
            if 'timestamp' in df.columns:
                latest_timestamp = df['timestamp'].max()
                time_diff = (datetime.now() - latest_timestamp).total_seconds()
                if time_diff < 300:  # 5分钟内
                    recency_score = 1.0
                elif time_diff < 1800:  # 30分钟内
                    recency_score = 0.8
                elif time_diff < 3600:  # 1小时内
                    recency_score = 0.6
                else:
                    recency_score = 0.3

            # 计算综合质量评分
            quality_score = (completeness_score * 0.4 +
                          consistency_score * 0.4 +
                          recency_score * 0.2)

            return min(quality_score, 1.0)

        except Exception as e:
            self.logger.error(f"计算质量评分失败: {e}")
            return 0.5

    async def _store_processed_data(self, df: pd.DataFrame, data_type: str, exchange: str, symbol: str):
        """存储处理后的数据"""
        try:
            # 这里应该实现实际的数据库存储逻辑
            # 由于是示例，我们只记录日志
            self.logger.info(f"存储处理后的数据: {data_type}, {exchange}/{symbol}, {len(df)} 条记录")

            # 缓存到Redis
            cache_key = f"processed_{data_type}_{exchange}_{symbol}"
            cache_data = {
                'data_type': data_type,
                'exchange': exchange,
                'symbol': symbol,
                'record_count': len(df),
                'latest_timestamp': df['timestamp'].max().isoformat() if 'timestamp' in df.columns else None,
                'quality_score': await self._calculate_quality_score(df, data_type),
                'processed_at': datetime.now().isoformat()
            }

            await redis_client.setex(cache_key, self.cache_ttl, json.dumps(cache_data))

        except Exception as e:
            self.logger.error(f"存储处理后的数据失败: {e}")

    def _update_processing_stats(self, result: ProcessingResult):
        """更新处理统计信息"""
        try:
            self.processing_stats['total_processed'] += result.processed_count
            self.processing_stats['total_errors'] += result.error_count
            self.processing_stats['total_repaired'] += result.repaired_count
            self.processing_stats['total_rejected'] += result.rejected_count

            # 更新平均质量评分
            total_processed = self.processing_stats['total_processed']
            if total_processed > 0:
                current_avg = self.processing_stats['avg_quality_score']
                new_score = result.quality_score
                self.processing_stats['avg_quality_score'] = \
                    (current_avg * (total_processed - result.processed_count) + new_score * result.processed_count) / total_processed

            # 更新平均处理时间
            if total_processed > 0:
                current_avg_time = self.processing_stats['avg_processing_time']
                new_time = result.processing_time
                self.processing_stats['avg_processing_time'] = \
                    (current_avg_time * (total_processed - result.processed_count) + new_time * result.processed_count) / total_processed

        except Exception as e:
            self.logger.error(f"更新处理统计信息失败: {e}")

    async def _processing_monitor_loop(self):
        """处理监控循环"""
        while self._running:
            try:
                # 检查处理队列积压
                await self._check_processing_backlog()

                # 清理过期的缓存
                await self._clean_expired_cache()

                # 记录处理统计
                await self._record_processing_stats()

                await asyncio.sleep(60)  # 每分钟检查一次

            except Exception as e:
                self.logger.error(f"处理监控失败: {e}")
                await asyncio.sleep(10)

    async def _check_processing_backlog(self):
        """检查处理积压"""
        try:
            # 这里应该检查是否有未处理的数据积压
            # 示例：检查处理延迟
            if self.processing_stats['avg_processing_time'] > 5.0:  # 超过5秒
                self.logger.warning(f"数据处理延迟过高: {self.processing_stats['avg_processing_time']:.2f}s")

            # 检查错误率
            total_processed = self.processing_stats['total_processed']
            if total_processed > 0:
                error_rate = self.processing_stats['total_errors'] / total_processed
                if error_rate > self.quality_thresholds['max_error_rate']:
                    self.logger.warning(f"数据处理错误率过高: {error_rate:.2%}")

        except Exception as e:
            self.logger.error(f"检查处理积压失败: {e}")

    async def _clean_expired_cache(self):
        """清理过期缓存"""
        try:
            # 清理超过24小时的缓存
            expired_keys = []
            for key in await redis_client.keys("processed_*"):
                ttl = await redis_client.ttl(key)
                if ttl > 86400:  # 超过24小时
                    expired_keys.append(key)

            if expired_keys:
                await redis_client.delete(*expired_keys)
                self.logger.info(f"清理了 {len(expired_keys)} 个过期缓存")

        except Exception as e:
            self.logger.error(f"清理过期缓存失败: {e}")

    async def _record_processing_stats(self):
        """记录处理统计"""
        try:
            # 记录到Prometheus
            metrics.record_processing_stats(
                total_processed=self.processing_stats['total_processed'],
                total_errors=self.processing_stats['total_errors'],
                total_repaired=self.processing_stats['total_repaired'],
                total_rejected=self.processing_stats['total_rejected'],
                avg_quality_score=self.processing_stats['avg_quality_score'],
                avg_processing_time=self.processing_stats['avg_processing_time']
            )

            # 缓存统计信息
            await redis_client.setex(
                "processing_stats",
                self.cache_ttl,
                json.dumps(self.processing_stats)
            )

        except Exception as e:
            self.logger.error(f"记录处理统计失败: {e}")

    async def get_processing_stats(self) -> Dict[str, Any]:
        """获取处理统计信息"""
        return self.processing_stats.copy()

    async def get_quality_report(self, exchange: str = None, symbol: str = None, hours: int = 24) -> Dict[str, Any]:
        """获取质量报告"""
        try:
            # 从数据库获取质量报告数据
            # 这里简化为返回当前统计信息
            report = {
                'report_period': f"{hours} hours",
                'generated_at': datetime.now().isoformat(),
                'exchange': exchange,
                'symbol': symbol,
                'processing_stats': self.processing_stats,
                'quality_summary': {
                    'avg_quality_score': self.processing_stats['avg_quality_score'],
                    'total_processed': self.processing_stats['total_processed'],
                    'error_rate': self.processing_stats['total_errors'] / max(self.processing_stats['total_processed'], 1),
                    'repair_rate': self.processing_stats['total_repaired'] / max(self.processing_stats['total_processed'], 1),
                    'rejection_rate': self.processing_stats['total_rejected'] / max(self.processing_stats['total_processed'], 1)
                },
                'recommendations': self._generate_quality_recommendations()
            }

            return report

        except Exception as e:
            self.logger.error(f"获取质量报告失败: {e}")
            return {}

    def _generate_quality_recommendations(self) -> List[str]:
        """生成质量改进建议"""
        recommendations = []

        try:
            stats = self.processing_stats

            # 检查错误率
            if stats['total_processed'] > 0:
                error_rate = stats['total_errors'] / stats['total_processed']
                if error_rate > self.quality_thresholds['max_error_rate']:
                    recommendations.append(f"错误率过高({error_rate:.2%})，建议检查数据源和验证规则")

            # 检查修复率
            repair_rate = stats['total_repaired'] / max(stats['total_processed'], 1)
            if repair_rate > self.quality_thresholds['max_repair_rate']:
                recommendations.append(f"修复率过高({repair_rate:.2%})，建议优化数据收集过程")

            # 检查平均质量评分
            if stats['avg_quality_score'] < self.quality_thresholds['min_quality_score']:
                recommendations.append(f"质量评分过低({stats['avg_quality_score']:.2f})，建议加强数据质量控制")

            # 检查处理时间
            if stats['avg_processing_time'] > 5.0:
                recommendations.append(f"处理时间过长({stats['avg_processing_time']:.2f}s)，建议优化处理算法")

        except Exception as e:
            self.logger.error(f"生成质量建议失败: {e}")

        return recommendations

    async def process_ticker_data(self, exchange: str, symbol: str, data: Dict) -> ProcessingResult:
        """处理Ticker数据"""
        # 实现Ticker数据处理逻辑
        # 结构与OHLCV处理类似
        pass

    async def process_orderbook_data(self, exchange: str, symbol: str, data: Dict) -> ProcessingResult:
        """处理订单簿数据"""
        # 实现订单簿数据处理逻辑
        pass

    async def process_trade_data(self, exchange: str, symbol: str, data: List[Dict]) -> ProcessingResult:
        """处理交易数据"""
        # 实现交易数据处理逻辑
        pass

    async def close(self):
        """关闭数据处理器"""
        self._running = False

        # 等待处理任务完成
        for task in self._processing_tasks:
            task.cancel()

        # 关闭线程池
        self._executor.shutdown(wait=True)

        self.logger.info("Data Processor 已关闭")

# 全局数据处理器实例
data_processor = DataProcessor()
```

## 接受标准

### 必须满足的条件
- [ ] 实现完整的数据清洗、验证、转换和增强流程
- [ ] 支持OHLCV、Ticker、OrderBook、Trade数据类型的处理
- [ ] 提供全面的验证规则和自动修复机制
- [ ] 实现实时数据质量评分和监控
- [ ] 支持技术指标计算和数据增强
- [ ] 提供处理统计和质量报告
- [ ] 实现高性能的并发处理架构
- [ ] 支持配置热更新和规则动态调整
- [ ] 通过性能测试和质量验证

### 性能要求
- **处理延迟**: 单批数据处理延迟 < 100ms
- **吞吐量**: 支持10,000+ 条记录/秒的处理速度
- **质量评分**: 数据质量评分准确度 > 95%
- **修复成功率**: 自动修复成功率 > 90%
- **内存使用**: 处理过程中的内存使用 < 1GB

### 技术规范
- 使用高性能数据处理库（Pandas/Polars）
- 实现异步处理架构
- 支持多线程并发处理
- 完整的错误处理和异常管理
- 符合数据安全和隐私要求

## 实现步骤

### 第一阶段：基础架构 (2天)
1. 设计数据处理核心架构
2. 实现基础数据清洗功能
3. 开发验证规则引擎
4. 实现缓存和性能优化

### 第二阶段：数据验证 (3天)
1. 实现各种验证规则
2. 开发自动修复算法
3. 实现数据转换逻辑
4. 添加质量评分机制

### 第三阶段：数据增强 (3天)
1. 实现技术指标计算
2. 开发数据增强功能
3. 添加市场状态分析
4. 实现质量分类系统

### 第四阶段：监控和报告 (2天)
1. 实现处理监控功能
2. 开发质量报告生成
3. 添加性能统计
4. 完善配置管理

## 交付物

### 文档
- 数据处理架构设计文档
- 验证规则配置文档
- 质量控制机制文档
- API接口文档

### 代码
- 数据处理器核心实现
- 验证规则引擎
- 质量评分系统
- 监控和报告模块
- 单元测试和集成测试

### 配置
- 验证规则配置文件
- 质量阈值配置
- 处理参数配置
- 监控配置

## 风险和依赖

### 技术风险
- 复杂的数据处理逻辑可能存在性能瓶颈
- 验证规则配置不当可能导致数据丢失
- 自动修复算法可能引入新的错误

### 依赖关系
- 依赖于Exchange Manager的原始数据
- 依赖于数据库性能和可用性
- 依赖于缓存系统的稳定性

### 缓解措施
- 实现分阶段的验证和处理
- 添加详细的日志记录和监控
- 提供手动干预和回滚机制

## 验收标准
- 数据处理准确可靠
- 质量控制有效及时
- 系统性能满足要求
- 报告生成完整准确
- 监控指标全面详细