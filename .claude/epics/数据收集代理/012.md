---
name: 数据缓存和性能优化
type: task
epic: 数据收集代理
status: open
priority: 2
created: 2025-09-25T20:20:35Z
estimated: 8 days
assigned: [待分配]
parallelizable: true
dependencies: ["002", "010"]
---

# 任务: 数据缓存和性能优化

## 任务描述
实现Redis缓存策略和系统性能优化，包括缓存设计、查询优化、资源管理和性能监控。通过智能化的缓存机制和性能优化策略，显著提升数据访问速度和系统整体性能，为高频交易系统提供低延迟的数据支持。

## 技术要求

### 核心架构设计

#### Cache Management System 组件
- **多层缓存**: L1内存缓存 + L2 Redis缓存 + L3数据库缓存
- **智能缓存策略**: 基于访问模式和数据热度的智能缓存
- **缓存失效管理**: 基于时间、事件和容量的缓存失效机制
- **性能监控**: 实时缓存命中率和性能指标监控
- **资源优化**: 内存使用优化和垃圾回收管理

#### 技术栈
- **缓存层**: Redis 6.0+ + Python内存缓存
- **数据库**: PostgreSQL + TimescaleDB
- **监控**: Prometheus + Grafana
- **优化工具**: SQLAlchemy查询优化 + 数据库索引优化
- **性能分析**: cProfile + memory_profiler

### 实现架构

```python
# src/cache/cache_manager.py
import asyncio
import time
import pickle
import hashlib
from typing import Dict, List, Optional, Any, Callable, Union, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
import threading
import weakref
from functools import wraps, lru_cache
from collections import defaultdict, OrderedDict
import logging
import json
from concurrent.futures import ThreadPoolExecutor
import psycopg2
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_, or_
import numpy as np

from ..core.config import config
from ..core.logger import get_logger
from ..core.metrics import metrics
from ..storage.postgresql import get_db_session
from ..storage.redis import redis_client
from ..models.cache import CacheEntry, CacheStats, PerformanceMetrics
from ..models.market_data import OHLCV, Ticker, OrderBook, Trade

class CacheLevel(Enum):
    """缓存级别枚举"""
    L1_MEMORY = "l1_memory"    # L1内存缓存
    L2_REDIS = "l2_redis"      # L2 Redis缓存
    L3_DATABASE = "l3_database"  # L3数据库缓存

class CacheStrategy(Enum):
    """缓存策略枚举"""
    LRU = "lru"           # 最近最少使用
    LFU = "lfu"           # 最不经常使用
    TTL = "ttl"           # 时间过期
    WRITE_THROUGH = "write_through"  # 写穿透
    WRITE_BACK = "write_back"        # 写回
    WRITE_AROUND = "write_around"    # 写绕过

class DataPriority(Enum):
    """数据优先级枚举"""
    CRITICAL = "critical"     # 关键数据（实时价格、仓位）
    HIGH = "high"             # 高优先级（分钟级数据）
    MEDIUM = "medium"         # 中等优先级（小时级数据）
    LOW = "low"              # 低优先级（历史数据）

@dataclass
class CacheConfig:
    """缓存配置"""
    level: CacheLevel
    strategy: CacheStrategy
    ttl: int  # 过期时间（秒）
    max_size: int  # 最大大小（条数）
    priority: DataPriority
    compress: bool = False
    serialize: bool = True

@dataclass
class CacheKey:
    """缓存键"""
    prefix: str
    exchange: str
    symbol: str
    data_type: str
    timeframe: str = None
    start_time: datetime = None
    end_time: datetime = None
    params: Dict[str, Any] = None

    def __hash__(self):
        return hash((
            self.prefix, self.exchange, self.symbol, self.data_type,
            self.timeframe, self.start_time, self.end_time,
            frozenset(self.params.items()) if self.params else None
        ))

    def __str__(self):
        parts = [
            self.prefix, self.exchange, self.symbol, self.data_type
        ]
        if self.timeframe:
            parts.append(self.timeframe)
        if self.params:
            param_str = hashlib.md5(json.dumps(self.params, sort_keys=True).encode()).hexdigest()[:8]
            parts.append(param_str)
        return ":".join(parts)

@dataclass
class CacheResult:
    """缓存结果"""
    found: bool
    data: Any = None
    source: CacheLevel = None
    ttl: int = 0
    size: int = 0
    access_time: float = 0

class CacheManager:
    """缓存管理器"""

    def __init__(self):
        self.logger = get_logger("cache_manager")
        self._executor = ThreadPoolExecutor(max_workers=4)
        self._lock = threading.RLock()

        # 缓存统计
        self.cache_stats = {
            CacheLevel.L1_MEMORY: CacheStats(),
            CacheLevel.L2_REDIS: CacheStats(),
            CacheLevel.L3_DATABASE: CacheStats()
        }

        # L1内存缓存
        self.l1_cache = OrderedDict()
        self.l1_max_size = 10000  # 最大缓存条数
        self.l1_cleanup_interval = 300  # 清理间隔（秒）
        self._last_l1_cleanup = time.time()

        # 缓存配置
        self.cache_configs = self._load_cache_configs()

        # 性能监控
        self.performance_metrics = PerformanceMetrics()
        self._monitoring_task = None
        self._running = False

        # 预热数据键
        self.preload_keys = set()

    def _load_cache_configs(self) -> Dict[str, CacheConfig]:
        """加载缓存配置"""
        return {
            'ohlcv_1m': CacheConfig(
                level=CacheLevel.L1_MEMORY,
                strategy=CacheStrategy.LRU,
                ttl=300,  # 5分钟
                max_size=5000,
                priority=DataPriority.CRITICAL
            ),
            'ohlcv_5m': CacheConfig(
                level=CacheLevel.L1_MEMORY,
                strategy=CacheStrategy.LRU,
                ttl=900,  # 15分钟
                max_size=3000,
                priority=DataPriority.HIGH
            ),
            'ohlcv_1h': CacheConfig(
                level=CacheLevel.L2_REDIS,
                strategy=CacheStrategy.TTL,
                ttl=3600,  # 1小时
                max_size=10000,
                priority=DataPriority.MEDIUM
            ),
            'ohlcv_1d': CacheConfig(
                level=CacheLevel.L2_REDIS,
                strategy=CacheStrategy.TTL,
                ttl=86400,  # 1天
                max_size=5000,
                priority=DataPriority.LOW
            ),
            'ticker': CacheConfig(
                level=CacheLevel.L1_MEMORY,
                strategy=CacheStrategy.LRU,
                ttl=60,  # 1分钟
                max_size=1000,
                priority=DataPriority.CRITICAL
            ),
            'orderbook': CacheConfig(
                level=CacheLevel.L1_MEMORY,
                strategy=CacheStrategy.LRU,
                ttl=30,  # 30秒
                max_size=500,
                priority=DataPriority.CRITICAL
            ),
            'trade': CacheConfig(
                level=CacheLevel.L2_REDIS,
                strategy=CacheStrategy.TTL,
                ttl=1800,  # 30分钟
                max_size=5000,
                priority=DataPriority.HIGH
            )
        }

    async def initialize(self):
        """初始化缓存管理器"""
        try:
            # 检查Redis连接
            await redis_client.ping()

            # 清理过期的缓存
            await self._cleanup_expired_cache()

            # 启动监控任务
            self._running = True
            self._monitoring_task = asyncio.create_task(self._cache_monitor_loop())

            # 预热关键数据
            await self._preload_critical_data()

            self.logger.info("Cache Manager 初始化完成")

        except Exception as e:
            self.logger.error(f"Cache Manager 初始化失败: {e}")
            raise

    async def get_data(self, cache_key: CacheKey, query_func: Callable = None) -> CacheResult:
        """获取缓存数据"""
        start_time = time.time()
        cache_result = CacheResult(found=False)

        try:
            # 生成缓存键字符串
            key_str = str(cache_key)

            # 获取缓存配置
            config = self._get_cache_config(cache_key)

            # L1缓存查找
            if config.level == CacheLevel.L1_MEMORY:
                cache_result = await self._get_l1_cache(key_str, config)
                if cache_result.found:
                    return cache_result

            # L2缓存查找
            if config.level in [CacheLevel.L2_REDIS, CacheLevel.L3_DATABASE]:
                cache_result = await self._get_l2_cache(key_str, config)
                if cache_result.found:
                    # 回填L1缓存
                    if config.level == CacheLevel.L2_REDIS and len(self.l1_cache) < self.l1_max_size:
                        await self._set_l1_cache(key_str, cache_result.data, config)
                    return cache_result

            # 缓存未命中，从数据库获取
            if query_func:
                try:
                    # 执行查询
                    data = await query_func()
                    access_time = time.time() - start_time

                    # 缓存数据
                    await self._set_cache(cache_key, data, config)

                    cache_result = CacheResult(
                        found=True,
                        data=data,
                        source=CacheLevel.L3_DATABASE,
                        ttl=config.ttl,
                        size=len(pickle.dumps(data)) if data else 0,
                        access_time=access_time
                    )

                    # 更新统计信息
                    self._update_cache_stats(CacheLevel.L3_DATABASE, False, access_time)

                except Exception as e:
                    self.logger.error(f"查询数据库失败: {e}")
                    return CacheResult(found=False)

            return cache_result

        except Exception as e:
            self.logger.error(f"获取缓存数据失败: {e}")
            return CacheResult(found=False)

    def _get_cache_config(self, cache_key: CacheKey) -> CacheConfig:
        """获取缓存配置"""
        # 根据数据类型和时间框架选择配置
        if cache_key.data_type == 'ohlcv':
            timeframe = cache_key.timeframe or '1m'
            config_key = f'ohlcv_{timeframe}'
        else:
            config_key = cache_key.data_type

        return self.cache_configs.get(config_key, self.cache_configs['ohlcv_1h'])

    async def _get_l1_cache(self, key: str, config: CacheConfig) -> CacheResult:
        """从L1缓存获取数据"""
        start_time = time.time()

        try:
            with self._lock:
                if key in self.l1_cache:
                    # LRU更新
                    value = self.l1_cache.pop(key)
                    self.l1_cache[key] = value

                    cache_result = CacheResult(
                        found=True,
                        data=value['data'],
                        source=CacheLevel.L1_MEMORY,
                        ttl=config.ttl,
                        size=len(pickle.dumps(value['data'])) if value['data'] else 0,
                        access_time=time.time() - start_time
                    )

                    # 更新统计信息
                    self._update_cache_stats(CacheLevel.L1_MEMORY, True, cache_result.access_time)

                    return cache_result

            return CacheResult(found=False)

        except Exception as e:
            self.logger.error(f"L1缓存获取失败: {e}")
            return CacheResult(found=False)

    async def _get_l2_cache(self, key: str, config: CacheConfig) -> CacheResult:
        """从L2缓存获取数据"""
        start_time = time.time()

        try:
            # 从Redis获取
            cached_data = await redis_client.get(key)
            if cached_data:
                # 反序列化数据
                if config.serialize:
                    data = pickle.loads(cached_data)
                else:
                    data = json.loads(cached_data)

                cache_result = CacheResult(
                    found=True,
                    data=data,
                    source=CacheLevel.L2_REDIS,
                    ttl=config.ttl,
                    size=len(cached_data),
                    access_time=time.time() - start_time
                )

                # 更新统计信息
                self._update_cache_stats(CacheLevel.L2_REDIS, True, cache_result.access_time)

                return cache_result

            return CacheResult(found=False)

        except Exception as e:
            self.logger.error(f"L2缓存获取失败: {e}")
            return CacheResult(found=False)

    async def _set_cache(self, cache_key: CacheKey, data: Any, config: CacheConfig):
        """设置缓存数据"""
        try:
            key_str = str(cache_key)

            # 设置L1缓存
            if config.level == CacheLevel.L1_MEMORY:
                await self._set_l1_cache(key_str, data, config)

            # 设置L2缓存
            if config.level in [CacheLevel.L2_REDIS, CacheLevel.L3_DATABASE]:
                await self._set_l2_cache(key_str, data, config)

        except Exception as e:
            self.logger.error(f"设置缓存数据失败: {e}")

    async def _set_l1_cache(self, key: str, data: Any, config: CacheConfig):
        """设置L1缓存"""
        try:
            with self._lock:
                # LRU清理
                if len(self.l1_cache) >= self.l1_max_size:
                    self.l1_cache.popitem(last=False)

                # 添加到缓存
                self.l1_cache[key] = {
                    'data': data,
                    'timestamp': time.time(),
                    'access_count': 0
                }

        except Exception as e:
            self.logger.error(f"L1缓存设置失败: {e}")

    async def _set_l2_cache(self, key: str, data: Any, config: CacheConfig):
        """设置L2缓存"""
        try:
            # 序列化数据
            if config.serialize:
                serialized_data = pickle.dumps(data)
            else:
                serialized_data = json.dumps(data, default=str)

            # 压缩数据
            if config.compress and len(serialized_data) > 1024:  # 超过1KB才压缩
                serialized_data = await self._compress_data(serialized_data)

            # 设置Redis缓存
            await redis_client.setex(key, config.ttl, serialized_data)

        except Exception as e:
            self.logger.error(f"L2缓存设置失败: {e}")

    async def _compress_data(self, data: bytes) -> bytes:
        """压缩数据"""
        try:
            import zlib
            return zlib.compress(data)
        except Exception:
            return data

    async def _update_cache_stats(self, level: CacheLevel, hit: bool, access_time: float):
        """更新缓存统计"""
        try:
            stats = self.cache_stats[level]

            stats.total_requests += 1
            stats.total_access_time += access_time

            if hit:
                stats.hits += 1
            else:
                stats.misses += 1

            # 更新命中率
            stats.hit_rate = stats.hits / stats.total_requests if stats.total_requests > 0 else 0

            # 更新平均访问时间
            stats.avg_access_time = stats.total_access_time / stats.total_requests

            # 记录指标
            metrics.record_cache_metrics(
                level=level.value,
                hit=hit,
                access_time=access_time,
                hit_rate=stats.hit_rate
            )

        except Exception as e:
            self.logger.error(f"更新缓存统计失败: {e}")

    async def invalidate_cache(self, cache_key: CacheKey):
        """失效缓存"""
        try:
            key_str = str(cache_key)

            # 从L1缓存删除
            with self._lock:
                if key_str in self.l1_cache:
                    del self.l1_cache[key_str]

            # 从L2缓存删除
            await redis_client.delete(key_str)

            self.logger.info(f"缓存已失效: {key_str}")

        except Exception as e:
            self.logger.error(f"失效缓存失败: {e}")

    async def invalidate_pattern(self, pattern: str):
        """按模式失效缓存"""
        try:
            # L1缓存失效
            with self._lock:
                keys_to_remove = [k for k in self.l1_cache.keys() if pattern in k]
                for key in keys_to_remove:
                    del self.l1_cache[key]

            # L2缓存失效
            redis_keys = await redis_client.keys(pattern)
            if redis_keys:
                await redis_client.delete(*redis_keys)

            self.logger.info(f"按模式失效缓存: {pattern}，删除了 {len(redis_keys)} 个缓存")

        except Exception as e:
            self.logger.error(f"按模式失效缓存失败: {e}")

    async def _cleanup_expired_cache(self):
        """清理过期缓存"""
        try:
            # L1缓存清理
            current_time = time.time()
            if current_time - self._last_l1_cleanup > self.l1_cleanup_interval:
                with self._lock:
                    expired_keys = []
                    for key, value in self.l1_cache.items():
                        if current_time - value['timestamp'] > 300:  # 5分钟过期
                            expired_keys.append(key)

                    for key in expired_keys:
                        del self.l1_cache[key]

                    self._last_l1_cleanup = current_time
                    self.logger.info(f"L1缓存清理完成，删除了 {len(expired_keys)} 个过期缓存")

            # L2缓存清理由Redis自动处理

        except Exception as e:
            self.logger.error(f"清理过期缓存失败: {e}")

    async def _preload_critical_data(self):
        """预热关键数据"""
        try:
            # 预热主要交易对的数据
            critical_symbols = ['BTC/USDT', 'ETH/USDT', 'BNB/USDT']
            exchanges = ['binance', 'okx', 'huobi']

            for exchange in exchanges:
                for symbol in critical_symbols:
                    # 预热实时价格
                    ticker_key = CacheKey(
                        prefix="ticker",
                        exchange=exchange,
                        symbol=symbol,
                        data_type="ticker"
                    )
                    self.preload_keys.add(str(ticker_key))

                    # 预热最近1分钟的OHLCV数据
                    ohlcv_key = CacheKey(
                        prefix="ohlcv",
                        exchange=exchange,
                        symbol=symbol,
                        data_type="ohlcv",
                        timeframe="1m"
                    )
                    self.preload_keys.add(str(ohlcv_key))

            self.logger.info(f"预热了 {len(self.preload_keys)} 个关键数据缓存")

        except Exception as e:
            self.logger.error(f"预热关键数据失败: {e}")

    async def _cache_monitor_loop(self):
        """缓存监控循环"""
        while self._running:
            try:
                # 清理过期缓存
                await self._cleanup_expired_cache()

                # 更新性能指标
                await self._update_performance_metrics()

                # 优化缓存策略
                await self._optimize_cache_strategy()

                await asyncio.sleep(60)  # 每分钟检查一次

            except Exception as e:
                self.logger.error(f"缓存监控失败: {e}")
                await asyncio.sleep(10)

    async def _update_performance_metrics(self):
        """更新性能指标"""
        try:
            # 计算各缓存级别的性能
            for level, stats in self.cache_stats.items():
                if stats.total_requests > 0:
                    # 计算性能分数
                    performance_score = (
                        stats.hit_rate * 0.6 +  # 命中率权重
                        (1.0 / max(stats.avg_access_time, 0.001)) * 0.4  # 响应时间权重
                    )

                    # 记录到Prometheus
                    metrics.record_cache_performance(
                        level=level.value,
                        hit_rate=stats.hit_rate,
                        avg_access_time=stats.avg_access_time,
                        performance_score=performance_score
                    )

            # 监控内存使用
            l1_memory_usage = len(self.l1_cache) / self.l1_max_size
            metrics.record_memory_usage('l1_cache', l1_memory_usage)

        except Exception as e:
            self.logger.error(f"更新性能指标失败: {e}")

    async def _optimize_cache_strategy(self):
        """优化缓存策略"""
        try:
            # 分析缓存访问模式
            access_patterns = self._analyze_access_patterns()

            # 调整缓存大小
            for level, pattern in access_patterns.items():
                if pattern['hit_rate'] < 0.5 and level == CacheLevel.L1_MEMORY:
                    # L1缓存命中率过低，考虑增加大小
                    self.l1_max_size = min(self.l1_max_size * 1.1, 20000)
                    self.logger.info(f"L1缓存命中率过低，调整大小为 {self.l1_max_size}")

                elif pattern['hit_rate'] > 0.9 and level == CacheLevel.L1_MEMORY:
                    # L1缓存命中率很高，考虑减少大小以节省内存
                    self.l1_max_size = max(self.l1_max_size * 0.9, 5000)
                    self.logger.info(f"L1缓存命中率很高，调整大小为 {self.l1_max_size}")

        except Exception as e:
            self.logger.error(f"优化缓存策略失败: {e}")

    def _analyze_access_patterns(self) -> Dict[CacheLevel, Dict[str, float]]:
        """分析访问模式"""
        patterns = {}

        for level, stats in self.cache_stats.items():
            patterns[level] = {
                'hit_rate': stats.hit_rate,
                'avg_access_time': stats.avg_access_time,
                'total_requests': stats.total_requests
            }

        return patterns

    async def get_cache_stats(self) -> Dict[str, Any]:
        """获取缓存统计信息"""
        try:
            stats = {}
            for level, cache_stats in self.cache_stats.items():
                stats[level.value] = {
                    'total_requests': cache_stats.total_requests,
                    'hits': cache_stats.hits,
                    'misses': cache_stats.misses,
                    'hit_rate': cache_stats.hit_rate,
                    'avg_access_time': cache_stats.avg_access_time,
                    'total_access_time': cache_stats.total_access_time
                }

            # 添加L1缓存大小信息
            stats['l1_memory'] = {
                'current_size': len(self.l1_cache),
                'max_size': self.l1_max_size,
                'usage_rate': len(self.l1_cache) / self.l1_max_size
            }

            return stats

        except Exception as e:
            self.logger.error(f"获取缓存统计失败: {e}")
            return {}

    async def optimize_database_queries(self):
        """优化数据库查询"""
        try:
            async with get_db_session() as session:
                # 分析慢查询
                slow_queries = await session.execute(text("""
                    SELECT query, mean_time, calls
                    FROM pg_stat_statements
                    WHERE mean_time > 100
                    ORDER BY mean_time DESC
                    LIMIT 10
                """))

                # 为常用查询创建索引
                await self._create_optimization_indexes(session)

                # 更新查询统计
                await self._update_query_stats(session)

        except Exception as e:
            self.logger.error(f"优化数据库查询失败: {e}")

    async def _create_optimization_indexes(self, session: AsyncSession):
        """创建优化索引"""
        try:
            # 为OHLCV表创建索引
            index_queries = [
                "CREATE INDEX IF NOT EXISTS idx_ohlcv_exchange_symbol_time ON ohlcv(exchange, symbol, timestamp)",
                "CREATE INDEX IF NOT EXISTS idx_ohlcv_timestamp ON ohlcv(timestamp)",
                "CREATE INDEX IF NOT EXISTS idx_ticker_exchange_symbol ON ticker(exchange, symbol)",
                "CREATE INDEX IF NOT EXISTS idx_trade_exchange_symbol_time ON trade(exchange, symbol, timestamp)"
            ]

            for query in index_queries:
                await session.execute(text(query))

            await session.commit()

        except Exception as e:
            self.logger.error(f"创建优化索引失败: {e}")

    async def _update_query_stats(self, session: AsyncSession):
        """更新查询统计"""
        try:
            # 重置统计信息
            await session.execute(text("SELECT pg_stat_statements_reset()"))
            await session.commit()

        except Exception as e:
            self.logger.error(f"更新查询统计失败: {e}")

    async def get_performance_report(self, hours: int = 24) -> Dict[str, Any]:
        """获取性能报告"""
        try:
            report = {
                'report_period': f"{hours} hours",
                'generated_at': datetime.now().isoformat(),
                'cache_stats': await self.get_cache_stats(),
                'performance_metrics': {
                    'l1_hit_rate': self.cache_stats[CacheLevel.L1_MEMORY].hit_rate,
                    'l2_hit_rate': self.cache_stats[CacheLevel.L2_REDIS].hit_rate,
                    'l1_avg_access_time': self.cache_stats[CacheLevel.L1_MEMORY].avg_access_time,
                    'l2_avg_access_time': self.cache_stats[CacheLevel.L2_REDIS].avg_access_time,
                    'l1_memory_usage': len(self.l1_cache) / self.l1_max_size
                },
                'optimization_recommendations': await self._generate_optimization_recommendations()
            }

            return report

        except Exception as e:
            self.logger.error(f"获取性能报告失败: {e}")
            return {}

    async def _generate_optimization_recommendations(self) -> List[str]:
        """生成优化建议"""
        recommendations = []

        try:
            # 基于缓存命中率
            l1_hit_rate = self.cache_stats[CacheLevel.L1_MEMORY].hit_rate
            l2_hit_rate = self.cache_stats[CacheLevel.L2_REDIS].hit_rate

            if l1_hit_rate < 0.5:
                recommendations.append("L1缓存命中率过低，建议增加L1缓存大小或优化缓存策略")

            if l2_hit_rate < 0.6:
                recommendations.append("L2缓存命中率较低，建议检查Redis配置或增加TTL时间")

            # 基于响应时间
            l1_avg_time = self.cache_stats[CacheLevel.L1_MEMORY].avg_access_time
            l2_avg_time = self.cache_stats[CacheLevel.L2_REDIS].avg_access_time

            if l1_avg_time > 0.01:  # 超过10ms
                recommendations.append("L1缓存响应时间过长，建议优化内存管理算法")

            if l2_avg_time > 0.05:  # 超过50ms
                recommendations.append("L2缓存响应时间过长，建议检查Redis性能或网络延迟")

            # 基于内存使用
            memory_usage = len(self.l1_cache) / self.l1_max_size
            if memory_usage > 0.9:
                recommendations.append("L1缓存内存使用率过高，建议清理过期数据或增加缓存大小")

        except Exception as e:
            self.logger.error(f"生成优化建议失败: {e}")

        return recommendations

    async def warm_up_cache(self, exchange: str, symbol: str, data_types: List[str]):
        """预热缓存"""
        try:
            for data_type in data_types:
                if data_type == 'ohlcv':
                    # 预热多个时间框架的OHLCV数据
                    timeframes = ['1m', '5m', '15m', '1h', '4h', '1d']
                    for timeframe in timeframes:
                        cache_key = CacheKey(
                            prefix="ohlcv",
                            exchange=exchange,
                            symbol=symbol,
                            data_type="ohlcv",
                            timeframe=timeframe
                        )
                        self.preload_keys.add(str(cache_key))

                elif data_type == 'ticker':
                    cache_key = CacheKey(
                        prefix="ticker",
                        exchange=exchange,
                        symbol=symbol,
                        data_type="ticker"
                    )
                    self.preload_keys.add(str(cache_key))

                elif data_type == 'orderbook':
                    cache_key = CacheKey(
                        prefix="orderbook",
                        exchange=exchange,
                        symbol=symbol,
                        data_type="orderbook"
                    )
                    self.preload_keys.add(str(cache_key))

            self.logger.info(f"预热缓存完成，添加了 {len(self.preload_keys)} 个预加载键")

        except Exception as e:
            self.logger.error(f"预热缓存失败: {e}")

    async def clear_all_cache(self):
        """清空所有缓存"""
        try:
            # 清空L1缓存
            with self._lock:
                self.l1_cache.clear()

            # 清空L2缓存
            await redis_client.flushdb()

            # 重置统计信息
            for stats in self.cache_stats.values():
                stats.reset()

            self.logger.info("所有缓存已清空")

        except Exception as e:
            self.logger.error(f"清空缓存失败: {e}")

    async def close(self):
        """关闭缓存管理器"""
        self._running = False

        if self._monitoring_task:
            self._monitoring_task.cancel()

        if self._executor:
            self._executor.shutdown(wait=True)

        self.logger.info("Cache Manager 已关闭")

# 全局缓存管理器实例
cache_manager = CacheManager()
```

## 接受标准

### 必须满足的条件
- [ ] 实现多层缓存架构（L1内存 + L2 Redis + L3数据库）
- [ ] 支持多种缓存策略（LRU、LFU、TTL、写穿透等）
- [ ] 提供智能缓存失效和管理机制
- [ ] 实现实时性能监控和优化
- [ ] 支持数据库查询优化和索引管理
- [ ] 提供缓存预热和性能报告
- [ ] 实现高并发和低延迟的缓存访问
- [ ] 支持配置热更新和动态调整
- [ ] 通过性能测试和负载测试

### 性能要求
- **L1缓存响应时间**: < 1ms (P95)
- **L2缓存响应时间**: < 10ms (P95)
- **整体缓存命中率**: > 85%
- **L1缓存命中率**: > 70%
- **并发处理能力**: 支持10,000+ QPS
- **内存使用效率**: 内存使用率 < 80%

### 技术规范
- 使用线程安全的缓存实现
- 实现异步处理架构
- 支持数据压缩和序列化
- 完整的错误处理和异常管理
- 符合数据安全和隐私要求

## 实现步骤

### 第一阶段：基础缓存架构 (2天)
1. 设计多层缓存架构
2. 实现L1内存缓存
3. 集成L2 Redis缓存
4. 实现基础缓存管理功能

### 第二阶段：缓存策略和优化 (3天)
1. 实现多种缓存策略
2. 开发智能缓存失效机制
3. 实现缓存预热功能
4. 添加性能监控指标

### 第三阶段：数据库优化 (2天)
1. 分析和优化慢查询
2. 创建数据库索引
3. 实现查询优化策略
4. 添加数据库性能监控

### 第四阶段：高级功能 (1天)
1. 实现缓存统计和分析
2. 开发性能报告生成
3. 添加优化建议功能
4. 完善配置管理

## 交付物

### 文档
- 缓存架构设计文档
- 性能优化指南
- 缓存配置文档
- 监控和告警文档

### 代码
- 缓存管理器核心实现
- 缓存策略模块
- 性能监控模块
- 数据库优化模块
- 报告生成器
- 单元测试和性能测试

### 配置
- 缓存策略配置文件
- 性能优化配置
- 监控阈值配置
- 数据库优化配置

## 风险和依赖

### 技术风险
- 缓存一致性问题
- 内存泄漏风险
- Redis性能瓶颈
- 数据库连接池管理

### 依赖关系
- 依赖于Redis服务器的稳定性
- 依赖于数据库性能和配置
- 依赖于网络连接质量

### 缓解措施
- 实现缓存一致性机制
- 添加内存监控和自动清理
- 提供Redis集群和备份方案
- 实现连接池和超时管理

## 验收标准
- 缓存性能达到要求指标
- 系统响应时间显著提升
- 数据库负载有效降低
- 监控指标全面准确
- 系统稳定性达到要求
- 性能优化效果明显