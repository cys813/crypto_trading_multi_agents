#!/usr/bin/env python3
"""
SOLæƒ…ç»ªåˆ†ææµ‹è¯•
ä¸“é—¨åˆ†æSolana(SOL)çš„å¸‚åœºæƒ…ç»ª
"""
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import json
import re
import time
from datetime import datetime, timedelta

def analyze_sentiment_keywords(text):
    """é’ˆå¯¹SOLä¼˜åŒ–çš„æƒ…ç»ªå…³é”®è¯åˆ†æ"""
    # SOL/Solanaç‰¹å®šçš„æ­£é¢å…³é”®è¯
    positive_keywords = [
        'bullish', 'rally', 'surge', 'moon', 'pump', 'breakout', 'bull', 'green',
        'up', 'rise', 'gain', 'profit', 'buy', 'hold', 'hodl', 'optimistic',
        'positive', 'strong', 'support', 'recovery', 'growth', 'institutional',
        'adoption', 'breakthrough', 'milestone', 'all-time high', 'ath',
        # SOLç‰¹å®šæ­£é¢è¯æ±‡
        'solana ecosystem', 'defi boom', 'nft surge', 'dapp growth', 'validator',
        'staking rewards', 'fast transaction', 'low fees', 'scalability',
        'phantom wallet', 'serum dex', 'raydium', 'orca', 'marinade',
        'jupiter aggregator', 'magic eden', 'metaplex', 'upgrade', 'innovation'
    ]
    
    # SOL/Solanaç‰¹å®šçš„è´Ÿé¢å…³é”®è¯
    negative_keywords = [
        'bearish', 'crash', 'dump', 'drop', 'fall', 'decline', 'bear', 'red',
        'down', 'loss', 'sell', 'panic', 'fear', 'pessimistic', 'negative',
        'weak', 'resistance', 'correction', 'bubble', 'scam', 'regulation',
        'ban', 'crackdown', 'risk', 'volatile', 'uncertainty', 'concern',
        # SOLç‰¹å®šè´Ÿé¢è¯æ±‡
        'network outage', 'downtime', 'congestion', 'validator issues',
        'centralization', 'restart', 'halt', 'bug', 'exploit', 'hack',
        'drain', 'rug pull', 'ftx collapse', 'alameda', 'bankruptcy'
    ]
    
    # ä¸­æ€§å…³é”®è¯
    neutral_keywords = [
        'stable', 'sideways', 'consolidation', 'analysis', 'technical',
        'fundamental', 'market', 'trading', 'volume', 'price', 'chart',
        'solana', 'sol', 'blockchain', 'ecosystem', 'dapp', 'defi', 'nft'
    ]
    
    text_lower = text.lower()
    
    positive_count = sum(1 for word in positive_keywords if word in text_lower)
    negative_count = sum(1 for word in negative_keywords if word in text_lower)
    neutral_count = sum(1 for word in neutral_keywords if word in text_lower)
    
    total_keywords = positive_count + negative_count + neutral_count
    
    if total_keywords == 0:
        return 0.0, "neutral", {"positive": 0, "negative": 0, "neutral": 0}
    
    # è®¡ç®—æƒ…ç»ªå¾—åˆ† (-1 åˆ° +1)
    sentiment_score = (positive_count - negative_count) / max(total_keywords, 1)
    
    # ç¡®å®šæƒ…ç»ªç±»åˆ«
    if sentiment_score > 0.2:
        sentiment_label = "positive"
    elif sentiment_score < -0.2:
        sentiment_label = "negative"
    else:
        sentiment_label = "neutral"
    
    keyword_counts = {
        "positive": positive_count,
        "negative": negative_count, 
        "neutral": neutral_count
    }
    
    return sentiment_score, sentiment_label, keyword_counts

def get_rss_news_sentiment():
    """è·å–RSSæ–°é—»å¹¶åˆ†æSOLæƒ…ç»ª"""
    print("ğŸ“° åˆ†æRSSæ–°é—»ä¸­çš„SOLæƒ…ç»ª...")
    
    rss_feeds = [
        ("Cointelegraph", "https://cointelegraph.com/rss"),
        ("CoinDesk", "https://www.coindesk.com/arc/outboundfeeds/rss/"),
        ("Decrypt", "https://decrypt.co/feed"),
        ("The Block", "https://theblock.co/rss.xml")
    ]
    
    all_articles = []
    total_sentiment = 0
    sentiment_counts = {"positive": 0, "negative": 0, "neutral": 0}
    
    for name, url in rss_feeds:
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; CryptoSentimentBot/1.0)'
            }
            
            request = urllib.request.Request(url, headers=headers)
            
            with urllib.request.urlopen(request, timeout=10) as response:
                content = response.read().decode('utf-8', errors='ignore')
            
            root = ET.fromstring(content)
            items = root.findall('.//item')
            
            for item in items[:30]:  # åˆ†ææœ€æ–°30ç¯‡æ–‡ç« 
                title_elem = item.find('title')
                desc_elem = item.find('description')
                
                title = title_elem.text if title_elem is not None else ""
                desc = desc_elem.text if desc_elem is not None else ""
                
                # æ¸…ç†HTMLæ ‡ç­¾
                title = re.sub(r'<[^>]+>', '', title)
                desc = re.sub(r'<[^>]+>', '', desc)
                
                text_content = f"{title} {desc}"
                
                # æ£€æŸ¥æ˜¯å¦ä¸SOLç›¸å…³
                if any(keyword in text_content.lower() for keyword in ['sol', 'solana']):
                    score, label, keywords = analyze_sentiment_keywords(text_content)
                    
                    article = {
                        'source': name,
                        'title': title[:100] + '...' if len(title) > 100 else title,
                        'sentiment_score': score,
                        'sentiment_label': label,
                        'keywords': keywords
                    }
                    
                    all_articles.append(article)
                    total_sentiment += score
                    sentiment_counts[label] += 1
                    
        except Exception as e:
            print(f"  âŒ {name}: {str(e)[:50]}...")
            continue
    
    avg_sentiment = total_sentiment / max(len(all_articles), 1)
    
    print(f"  ğŸ“Š åˆ†æäº† {len(all_articles)} ç¯‡SOLç›¸å…³æ–‡ç« ")
    print(f"  ğŸ“ˆ æ­£é¢: {sentiment_counts['positive']} ç¯‡")
    print(f"  ğŸ“‰ è´Ÿé¢: {sentiment_counts['negative']} ç¯‡") 
    print(f"  ğŸ˜ ä¸­æ€§: {sentiment_counts['neutral']} ç¯‡")
    print(f"  ğŸ¯ å¹³å‡æƒ…ç»ªå¾—åˆ†: {avg_sentiment:.3f}")
    
    return {
        'articles': all_articles,
        'average_sentiment': avg_sentiment,
        'sentiment_counts': sentiment_counts,
        'total_articles': len(all_articles)
    }

def get_social_media_sentiment():
    """è·å–ç¤¾äº¤åª’ä½“æ•°æ®å¹¶åˆ†æSOLæƒ…ç»ª"""
    print("\nğŸ“± åˆ†æç¤¾äº¤åª’ä½“ä¸­çš„SOLæƒ…ç»ª...")
    
    # Redditæ•°æ® - SOLç›¸å…³subreddit
    reddit_sentiment = {"positive": 0, "negative": 0, "neutral": 0}
    reddit_total_score = 0
    reddit_posts = 0
    
    reddit_sources = [
        "https://www.reddit.com/r/solana/hot.json",
        "https://www.reddit.com/r/cryptocurrency/hot.json"
    ]
    
    for reddit_url in reddit_sources:
        try:
            headers = {'User-Agent': 'CryptoSentimentBot/1.0'}
            request = urllib.request.Request(reddit_url, headers=headers)
            
            with urllib.request.urlopen(request, timeout=10) as response:
                content = response.read().decode('utf-8')
            
            data = json.loads(content)
            posts = data.get('data', {}).get('children', [])
            
            for post in posts[:20]:
                post_data = post.get('data', {})
                title = post_data.get('title', '')
                selftext = post_data.get('selftext', '')
                
                text_content = f"{title} {selftext}"
                
                if any(keyword in text_content.lower() for keyword in ['sol', 'solana']):
                    score, label, _ = analyze_sentiment_keywords(text_content)
                    reddit_total_score += score
                    reddit_sentiment[label] += 1
                    reddit_posts += 1
                    
        except Exception as e:
            print(f"  âŒ Redditè·å–å¤±è´¥: {str(e)[:50]}...")
            continue
            
    reddit_avg = reddit_total_score / max(reddit_posts, 1)
    print(f"  ğŸŸ  Reddit: {reddit_posts} ä¸ªç›¸å…³å¸–å­, å¹³å‡æƒ…ç»ª: {reddit_avg:.3f}")
    
    # CryptoCompareæ–°é—»API
    crypto_compare_sentiment = {"positive": 0, "negative": 0, "neutral": 0}
    crypto_compare_total_score = 0
    crypto_compare_articles = 0
    
    try:
        url = "https://min-api.cryptocompare.com/data/v2/news/?lang=EN"
        headers = {'User-Agent': 'CryptoSentimentBot/1.0'}
        request = urllib.request.Request(url, headers=headers)
        
        with urllib.request.urlopen(request, timeout=10) as response:
            content = response.read().decode('utf-8')
        
        data = json.loads(content)
        articles = data.get('Data', [])
        
        for article in articles[:50]:
            title = article.get('title', '')
            body = article.get('body', '')
            
            text_content = f"{title} {body}"
            
            if any(keyword in text_content.lower() for keyword in ['sol', 'solana']):
                score, label, _ = analyze_sentiment_keywords(text_content)
                crypto_compare_total_score += score
                crypto_compare_sentiment[label] += 1
                crypto_compare_articles += 1
        
        crypto_compare_avg = crypto_compare_total_score / max(crypto_compare_articles, 1)
        print(f"  ğŸ”· CryptoCompare: {crypto_compare_articles} ç¯‡ç›¸å…³æ–‡ç« , å¹³å‡æƒ…ç»ª: {crypto_compare_avg:.3f}")
        
    except Exception as e:
        print(f"  âŒ CryptoCompareè·å–å¤±è´¥: {str(e)[:50]}...")
        crypto_compare_avg = 0
        crypto_compare_articles = 0
    
    # ç»¼åˆç¤¾äº¤åª’ä½“æƒ…ç»ª
    total_social_posts = reddit_posts + crypto_compare_articles
    combined_sentiment = {
        "positive": reddit_sentiment["positive"] + crypto_compare_sentiment["positive"],
        "negative": reddit_sentiment["negative"] + crypto_compare_sentiment["negative"],
        "neutral": reddit_sentiment["neutral"] + crypto_compare_sentiment["neutral"]
    }
    
    combined_avg = (reddit_total_score + crypto_compare_total_score) / max(total_social_posts, 1)
    
    return {
        'reddit': {'sentiment': reddit_avg, 'posts': reddit_posts, 'counts': reddit_sentiment},
        'crypto_compare': {'sentiment': crypto_compare_avg, 'articles': crypto_compare_articles, 'counts': crypto_compare_sentiment},
        'combined': {'sentiment': combined_avg, 'total': total_social_posts, 'counts': combined_sentiment}
    }

def generate_sol_sentiment_insights(news_data, social_data):
    """ç”ŸæˆSOLç‰¹å®šçš„æƒ…ç»ªåˆ†ææ´å¯Ÿ"""
    news_sentiment = news_data['average_sentiment']
    social_sentiment = social_data['combined']['sentiment']
    
    # ç»¼åˆæƒ…ç»ªå¾—åˆ† (æ–°é—»70%æƒé‡ï¼Œç¤¾äº¤åª’ä½“30%æƒé‡)
    overall_sentiment = (news_sentiment * 0.7) + (social_sentiment * 0.3)
    
    # SOLç‰¹å®šçš„æƒ…ç»ªç­‰çº§
    if overall_sentiment > 0.4:
        sentiment_level = "ğŸš€ æåº¦ä¹è§‚"
        market_mood = "SOLç”Ÿæ€ç³»ç»Ÿè“¬å‹ƒå‘å±•ï¼ŒæŠ•èµ„è€…ä¿¡å¿ƒçˆ†æ£š"
    elif overall_sentiment > 0.1:
        sentiment_level = "ğŸ“ˆ ä¹è§‚"
        market_mood = "SOLå‘å±•åŠ¿å¤´è‰¯å¥½ï¼Œç¤¾åŒºæ´»è·ƒåº¦é«˜"
    elif overall_sentiment > -0.1:
        sentiment_level = "ğŸ˜ ä¸­æ€§"
        market_mood = "SOLå¸‚åœºæƒ…ç»ªå¹³ç¨³ï¼Œç­‰å¾…å‚¬åŒ–å› ç´ "
    elif overall_sentiment > -0.4:
        sentiment_level = "ğŸ“‰ æ‚²è§‚"
        market_mood = "SOLé¢ä¸´æŒ‘æˆ˜ï¼ŒæŠ•èµ„è€…ä¿¡å¿ƒä¸è¶³"
    else:
        sentiment_level = "ğŸ’€ æåº¦æ‚²è§‚"
        market_mood = "SOLé­é‡ä¸¥é‡å›°éš¾ï¼Œææ…Œæƒ…ç»ªè”“å»¶"
    
    # ç”ŸæˆSOLç‰¹å®šæ´å¯Ÿ
    insights = []
    
    # æ–°é—»vsç¤¾äº¤åª’ä½“å¯¹æ¯”
    if abs(news_sentiment - social_sentiment) > 0.3:
        if news_sentiment > social_sentiment:
            insights.append("ğŸ“° ä¸»æµåª’ä½“å¯¹SOLæŠ¥é“æ¯”ç¤¾åŒºè®¨è®ºæ›´ç§¯æ")
        else:
            insights.append("ğŸ“± SOLç¤¾åŒºæƒ…ç»ªæ¯”ä¸»æµåª’ä½“æŠ¥é“æ›´ä¹è§‚")
    else:
        insights.append("ğŸ“Š SOLåœ¨æ–°é—»åª’ä½“å’Œç¤¾åŒºä¸­çš„æƒ…ç»ªåŸºæœ¬ä¸€è‡´")
    
    # æ•°æ®é‡è¯„ä¼°
    total_data_points = news_data['total_articles'] + social_data['combined']['total']
    if total_data_points > 30:
        insights.append(f"ğŸ“ˆ SOLç›¸å…³æ•°æ®å……è¶³({total_data_points}ä¸ªæ•°æ®ç‚¹)ï¼Œåˆ†æç»“æœå¯é ")
    elif total_data_points > 15:
        insights.append(f"ğŸ“Š SOLç›¸å…³æ•°æ®é€‚ä¸­({total_data_points}ä¸ªæ•°æ®ç‚¹)ï¼Œåˆ†æç»“æœåŸºæœ¬å¯ä¿¡")
    elif total_data_points > 5:
        insights.append(f"âš ï¸ SOLç›¸å…³æ•°æ®æœ‰é™({total_data_points}ä¸ªæ•°æ®ç‚¹)ï¼Œå…³æ³¨åº¦ç›¸å¯¹è¾ƒä½")
    else:
        insights.append(f"ğŸ” SOLç›¸å…³è®¨è®ºå¾ˆå°‘({total_data_points}ä¸ªæ•°æ®ç‚¹)ï¼Œå¸‚åœºå…³æ³¨åº¦ä¸é«˜")
    
    # SOLç‰¹å®šçš„æƒ…ç»ªåˆ†å¸ƒåˆ†æ
    total_positive = news_data['sentiment_counts']['positive'] + social_data['combined']['counts']['positive']
    total_negative = news_data['sentiment_counts']['negative'] + social_data['combined']['counts']['negative']
    
    if total_positive > total_negative * 2:
        insights.append("ğŸŸ¢ SOLæ­£é¢å†…å®¹å ä¸»å¯¼ï¼Œç”Ÿæ€ç³»ç»Ÿå‘å±•è·å¾—è®¤å¯")
    elif total_negative > total_positive * 2:
        insights.append("ğŸ”´ SOLé¢ä¸´è¾ƒå¤šè´¨ç–‘ï¼ŒæŠ€æœ¯æˆ–å¸‚åœºæŒ‘æˆ˜æ˜æ˜¾")
    else:
        insights.append("ğŸŸ¡ SOLç›¸å…³è§‚ç‚¹åˆ†åŒ–ï¼Œå¸‚åœºæ€åº¦è°¨æ…")
    
    # SOLç”Ÿæ€ç³»ç»Ÿç‰¹å®šåˆ†æ
    if news_data['total_articles'] > 0:
        # æ£€æŸ¥æ˜¯å¦æœ‰ç”Ÿæ€ç³»ç»Ÿç›¸å…³çš„ç‰¹å®šè¯é¢˜
        ecosystem_topics = []
        for article in news_data['articles']:
            title_lower = article['title'].lower()
            if any(term in title_lower for term in ['nft', 'defi', 'dapp', 'ecosystem']):
                ecosystem_topics.append("ç”Ÿæ€ç³»ç»Ÿå‘å±•")
            if any(term in title_lower for term in ['upgrade', 'validator', 'network']):
                ecosystem_topics.append("ç½‘ç»œå‡çº§")
            if any(term in title_lower for term in ['partnership', 'integration']):
                ecosystem_topics.append("åˆä½œä¼™ä¼´å…³ç³»")
        
        if ecosystem_topics:
            unique_topics = list(set(ecosystem_topics))
            insights.append(f"ğŸ”§ ä¸»è¦å…³æ³¨ç‚¹: {', '.join(unique_topics)}")
    
    return {
        'overall_sentiment': overall_sentiment,
        'sentiment_level': sentiment_level,
        'market_mood': market_mood,
        'insights': insights
    }

def main():
    """ä¸»åˆ†æå‡½æ•°"""
    print("ğŸŸ£ SOL(Solana)æœ¬å‘¨æƒ…ç»ªåˆ†æ")
    print("=" * 60)
    print(f"ğŸ“… åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸ” æ•°æ®æ¥æº: RSSæ–°é—» + Reddit(r/solana + r/cryptocurrency) + CryptoCompare API")
    print("ğŸ¯ ä¸“é—¨é’ˆå¯¹: SOL/Solanaç”Ÿæ€ç³»ç»Ÿ")
    print()
    
    # è·å–æ–°é—»æƒ…ç»ª
    start_time = time.time()
    news_data = get_rss_news_sentiment()
    news_time = time.time() - start_time
    
    # è·å–ç¤¾äº¤åª’ä½“æƒ…ç»ª
    start_time = time.time()  
    social_data = get_social_media_sentiment()
    social_time = time.time() - start_time
    
    # ç”Ÿæˆç»¼åˆåˆ†æ
    analysis = generate_sol_sentiment_insights(news_data, social_data)
    
    # è¾“å‡ºåˆ†ææŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š SOLæƒ…ç»ªåˆ†ææŠ¥å‘Š")
    print("=" * 60)
    
    print(f"ğŸ¯ ç»¼åˆæƒ…ç»ªå¾—åˆ†: {analysis['overall_sentiment']:.3f}")
    print(f"ğŸ“ˆ æƒ…ç»ªç­‰çº§: {analysis['sentiment_level']}")
    print(f"ğŸ’­ å¸‚åœºå¿ƒæ€: {analysis['market_mood']}")
    
    print(f"\nğŸ“° æ–°é—»åª’ä½“æƒ…ç»ª:")
    print(f"  ğŸ“Š å¹³å‡å¾—åˆ†: {news_data['average_sentiment']:.3f}")
    print(f"  ğŸ“„ åˆ†ææ–‡ç« : {news_data['total_articles']} ç¯‡")
    print(f"  ğŸ“ˆ æ­£é¢: {news_data['sentiment_counts']['positive']} ç¯‡")
    print(f"  ğŸ“‰ è´Ÿé¢: {news_data['sentiment_counts']['negative']} ç¯‡")
    print(f"  ğŸ˜ ä¸­æ€§: {news_data['sentiment_counts']['neutral']} ç¯‡")
    
    print(f"\nğŸ“± ç¤¾äº¤åª’ä½“æƒ…ç»ª:")
    print(f"  ğŸ“Š å¹³å‡å¾—åˆ†: {social_data['combined']['sentiment']:.3f}")
    print(f"  ğŸ’¬ åˆ†æå†…å®¹: {social_data['combined']['total']} æ¡")
    print(f"  ğŸ“ˆ æ­£é¢: {social_data['combined']['counts']['positive']} æ¡")
    print(f"  ğŸ“‰ è´Ÿé¢: {social_data['combined']['counts']['negative']} æ¡")
    print(f"  ğŸ˜ ä¸­æ€§: {social_data['combined']['counts']['neutral']} æ¡")
    
    print(f"\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
    for i, insight in enumerate(analysis['insights'], 1):
        print(f"  {i}. {insight}")
    
    print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
    print(f"  ğŸ“° æ–°é—»åˆ†æè€—æ—¶: {news_time:.2f}ç§’")
    print(f"  ğŸ“± ç¤¾äº¤åˆ†æè€—æ—¶: {social_time:.2f}ç§’")
    print(f"  ğŸ¯ æ€»è®¡SOLæ•°æ®ç‚¹: {news_data['total_articles'] + social_data['combined']['total']} ä¸ª")
    
    # æ˜¾ç¤ºéƒ¨åˆ†æ–‡ç« æ ‡é¢˜ä½œä¸ºç¤ºä¾‹
    if news_data['articles']:
        print(f"\nğŸ“ æœ€æ–°SOLç›¸å…³æ–‡ç« ç¤ºä¾‹:")
        for i, article in enumerate(news_data['articles'][:5], 1):
            sentiment_emoji = "ğŸ“ˆ" if article['sentiment_score'] > 0.1 else "ğŸ“‰" if article['sentiment_score'] < -0.1 else "ğŸ˜"
            print(f"  {i}. {sentiment_emoji} {article['title']} ({article['source']})")
    else:
        print(f"\nğŸ“ æ³¨æ„: æœªæ‰¾åˆ°SOLç›¸å…³çš„æœ€æ–°æ–‡ç« ")
        print(f"  ğŸ’¡ è¿™å¯èƒ½è¡¨æ˜ï¼š")
        print(f"     â€¢ SOLå½“å‰ä¸æ˜¯ä¸»è¦å…³æ³¨ç„¦ç‚¹")
        print(f"     â€¢ å¸‚åœºå¤„äºç›¸å¯¹å¹³é™æœŸ")
        print(f"     â€¢ éœ€è¦å…³æ³¨å…¶ä»–ä¿¡æ¯æº")
    
    print(f"\nğŸ”— æ•°æ®æºçŠ¶æ€:")
    print(f"  âœ… RSSæ–°é—»æº: æ­£å¸¸å·¥ä½œ")
    if social_data['reddit']['posts'] > 0:
        print(f"  âœ… Reddit API: æ­£å¸¸å·¥ä½œ") 
    else:
        print(f"  âš ï¸ Reddit API: æ•°æ®æœ‰é™æˆ–SOLè®¨è®ºè¾ƒå°‘")
        
    if social_data['crypto_compare']['articles'] > 0:
        print(f"  âœ… CryptoCompare API: æ­£å¸¸å·¥ä½œ")
    else:
        print(f"  âš ï¸ CryptoCompare API: æœªæ‰¾åˆ°SOLç›¸å…³å†…å®¹")
    
    # SOLç‰¹å®šçš„å¸‚åœºå»ºè®®
    print(f"\nğŸŸ£ SOLç‰¹å®šæŠ•èµ„å‚è€ƒ:")
    total_data = news_data['total_articles'] + social_data['combined']['total']
    sentiment_score = analysis['overall_sentiment']
    
    if total_data < 10:
        print(f"  ğŸ“Š å…³æ³¨åº¦: å½“å‰SOLå…³æ³¨åº¦è¾ƒä½ï¼Œé€‚åˆä½è°ƒå¸ƒå±€")
    elif sentiment_score > 0.2:
        print(f"  ğŸ“ˆ ç§¯æä¿¡å·: SOLæƒ…ç»ªç§¯æï¼Œä½†éœ€æ³¨æ„å›è°ƒé£é™©")  
    elif sentiment_score < -0.2:
        print(f"  ğŸ“‰ è°¨æ…ä¿¡å·: SOLæƒ…ç»ªåè´Ÿé¢ï¼Œç­‰å¾…ä¼ç¨³ä¿¡å·")
    else:
        print(f"  ğŸ˜ ä¸­æ€§ä¿¡å·: SOLæƒ…ç»ªå¹³ç¨³ï¼Œå¯é€¢ä½å…³æ³¨")
    
    print(f"\nâš ï¸ SOLæŠ•èµ„æé†’:")
    print(f"  â€¢ SOLä½œä¸ºæ™ºèƒ½åˆçº¦å¹³å°ï¼Œå…³æ³¨ç”Ÿæ€ç³»ç»Ÿå‘å±•")
    print(f"  â€¢ æ³¨æ„ç½‘ç»œç¨³å®šæ€§å’Œå»ä¸­å¿ƒåŒ–ç¨‹åº¦") 
    print(f"  â€¢ ç›‘æ§DeFiã€NFTã€GameFiç­‰ç”Ÿæ€é¡¹ç›®è¡¨ç°")
    print(f"  â€¢ æœ¬åˆ†æä¸æ„æˆæŠ•èµ„å»ºè®®ï¼Œè¯·ç†æ€§åˆ¤æ–­é£é™©")

if __name__ == "__main__":
    main()