#!/usr/bin/env python3
"""
åŠ å¯†è´§å¸æƒ…ç»ªåˆ†æ - é™å®šæœ€è¿‘2å‘¨æ–°é—»æ•°æ®
ä¸“é—¨åˆ†æBTCå’ŒSOLçš„æœ€è¿‘2å‘¨å¸‚åœºæƒ…ç»ª
"""
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import json
import re
import time
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime

def is_within_two_weeks(date_str):
    """æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨æœ€è¿‘2å‘¨å†…"""
    if not date_str:
        return True  # å¦‚æœæ²¡æœ‰æ—¥æœŸä¿¡æ¯ï¼Œé»˜è®¤åŒ…å«
    
    try:
        # å°è¯•è§£æå„ç§æ—¥æœŸæ ¼å¼
        if 'T' in date_str and 'Z' in date_str:
            # ISO 8601æ ¼å¼: 2025-01-01T12:00:00Z
            article_date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        elif '+' in date_str or '-' in date_str[-6:]:
            # å¸¦æ—¶åŒºçš„ISOæ ¼å¼
            article_date = datetime.fromisoformat(date_str)
        else:
            # RFC 2822æ ¼å¼ (RSSå¸¸ç”¨): Mon, 01 Jan 2025 12:00:00 GMT
            article_date = parsedate_to_datetime(date_str)
        
        # å»é™¤æ—¶åŒºä¿¡æ¯è¿›è¡Œæ¯”è¾ƒ
        if article_date.tzinfo:
            article_date = article_date.replace(tzinfo=None)
        
        # è®¡ç®—æ˜¯å¦åœ¨æœ€è¿‘2å‘¨å†…
        two_weeks_ago = datetime.now() - timedelta(days=14)
        return article_date >= two_weeks_ago
        
    except Exception as e:
        print(f"    âš ï¸ æ—¥æœŸè§£æå¤±è´¥: {date_str[:30]}... - {str(e)[:50]}")
        return True  # è§£æå¤±è´¥é»˜è®¤åŒ…å«

def analyze_sentiment_keywords(text, currency="BTC"):
    """æƒ…ç»ªå…³é”®è¯åˆ†æï¼Œæ”¯æŒä¸åŒåŠ å¯†è´§å¸"""
    
    # é€šç”¨æ­£é¢å…³é”®è¯
    positive_keywords = [
        'bullish', 'rally', 'surge', 'moon', 'pump', 'breakout', 'bull', 'green',
        'up', 'rise', 'gain', 'profit', 'buy', 'hold', 'hodl', 'optimistic',
        'positive', 'strong', 'support', 'recovery', 'growth', 'institutional',
        'adoption', 'breakthrough', 'milestone', 'all-time high', 'ath', 'soar'
    ]
    
    # é€šç”¨è´Ÿé¢å…³é”®è¯
    negative_keywords = [
        'bearish', 'crash', 'dump', 'drop', 'fall', 'decline', 'bear', 'red',
        'down', 'loss', 'sell', 'panic', 'fear', 'pessimistic', 'negative',
        'weak', 'resistance', 'correction', 'bubble', 'scam', 'regulation',
        'ban', 'crackdown', 'risk', 'volatile', 'uncertainty', 'concern', 'plunge'
    ]
    
    # è´§å¸ç‰¹å®šå…³é”®è¯
    if currency.upper() == "SOL":
        positive_keywords.extend([
            'solana ecosystem', 'defi boom', 'nft surge', 'dapp growth', 'validator',
            'staking rewards', 'fast transaction', 'low fees', 'scalability',
            'phantom wallet', 'serum dex', 'raydium', 'orca', 'marinade',
            'jupiter aggregator', 'magic eden', 'metaplex', 'upgrade', 'innovation'
        ])
        negative_keywords.extend([
            'network outage', 'downtime', 'congestion', 'validator issues',
            'centralization', 'restart', 'halt', 'bug', 'exploit', 'hack',
            'drain', 'rug pull', 'ftx collapse', 'alameda'
        ])
    
    # ä¸­æ€§å…³é”®è¯
    neutral_keywords = [
        'stable', 'sideways', 'consolidation', 'analysis', 'technical',
        'fundamental', 'market', 'trading', 'volume', 'price', 'chart'
    ]
    
    text_lower = text.lower()
    
    positive_count = sum(1 for word in positive_keywords if word in text_lower)
    negative_count = sum(1 for word in negative_keywords if word in text_lower)
    neutral_count = sum(1 for word in neutral_keywords if word in text_lower)
    
    total_keywords = positive_count + negative_count + neutral_count
    
    if total_keywords == 0:
        return 0.0, "neutral", {"positive": 0, "negative": 0, "neutral": 0}
    
    # è®¡ç®—æƒ…ç»ªå¾—åˆ† (-1 åˆ° +1)
    sentiment_score = (positive_count - negative_count) / max(total_keywords, 1)
    
    # ç¡®å®šæƒ…ç»ªç±»åˆ«
    if sentiment_score > 0.2:
        sentiment_label = "positive"
    elif sentiment_score < -0.2:
        sentiment_label = "negative"
    else:
        sentiment_label = "neutral"
    
    keyword_counts = {
        "positive": positive_count,
        "negative": negative_count, 
        "neutral": neutral_count
    }
    
    return sentiment_score, sentiment_label, keyword_counts

def get_rss_news_sentiment_2weeks(currency="BTC"):
    """è·å–æœ€è¿‘2å‘¨çš„RSSæ–°é—»å¹¶åˆ†ææƒ…ç»ª"""
    print(f"ğŸ“° åˆ†ææœ€è¿‘2å‘¨RSSæ–°é—»ä¸­çš„{currency}æƒ…ç»ª...")
    
    rss_feeds = [
        ("Cointelegraph", "https://cointelegraph.com/rss"),
        ("CoinDesk", "https://www.coindesk.com/arc/outboundfeeds/rss/"),
        ("Decrypt", "https://decrypt.co/feed"),
        ("The Block", "https://theblock.co/rss.xml"),
        ("CryptoSlate", "https://cryptoslate.com/feed/"),
        ("BeInCrypto", "https://beincrypto.com/feed/")
    ]
    
    all_articles = []
    total_sentiment = 0
    sentiment_counts = {"positive": 0, "negative": 0, "neutral": 0}
    date_filtered_count = 0
    currency_filtered_count = 0
    
    # è®¾ç½®æœç´¢å…³é”®è¯
    if currency.upper() == "SOL":
        search_keywords = ['sol', 'solana']
    else:
        search_keywords = ['btc', 'bitcoin']
    
    for name, url in rss_feeds:
        print(f"  ğŸ” æ£€æŸ¥ {name}...")
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; CryptoSentimentBot/1.0)'
            }
            
            request = urllib.request.Request(url, headers=headers)
            
            with urllib.request.urlopen(request, timeout=15) as response:
                content = response.read().decode('utf-8', errors='ignore')
            
            root = ET.fromstring(content)
            items = root.findall('.//item')
            
            source_articles = 0
            source_filtered_by_date = 0
            source_filtered_by_currency = 0
            
            for item in items:
                # è·å–æ–‡ç« ä¿¡æ¯
                title_elem = item.find('title')
                desc_elem = item.find('description')
                date_elem = item.find('pubDate')
                
                title = title_elem.text if title_elem is not None else ""
                desc = desc_elem.text if desc_elem is not None else ""
                pub_date = date_elem.text if date_elem is not None else ""
                
                # æ¸…ç†HTMLæ ‡ç­¾
                title = re.sub(r'<[^>]+>', '', title)
                desc = re.sub(r'<[^>]+>', '', desc)
                
                text_content = f"{title} {desc}"
                
                # é¦–å…ˆæ£€æŸ¥æ—¥æœŸè¿‡æ»¤
                if not is_within_two_weeks(pub_date):
                    source_filtered_by_date += 1
                    continue
                
                # ç„¶åæ£€æŸ¥è´§å¸ç›¸å…³æ€§
                if any(keyword in text_content.lower() for keyword in search_keywords):
                    source_filtered_by_currency += 1
                    score, label, keywords = analyze_sentiment_keywords(text_content, currency)
                    
                    article = {
                        'source': name,
                        'title': title[:120] + '...' if len(title) > 120 else title,
                        'sentiment_score': score,
                        'sentiment_label': label,
                        'keywords': keywords,
                        'pub_date': pub_date,
                        'parsed_date': pub_date[:20] if pub_date else "Unknown"
                    }
                    
                    all_articles.append(article)
                    total_sentiment += score
                    sentiment_counts[label] += 1
                    source_articles += 1
            
            print(f"    ğŸ“Š æ‰¾åˆ° {source_articles} ç¯‡{currency}ç›¸å…³æ–‡ç« ")
            if source_filtered_by_date > 0:
                print(f"    ğŸ“… è¿‡æ»¤æ‰ {source_filtered_by_date} ç¯‡è¿‡æœŸæ–‡ç« ")
            
            date_filtered_count += source_filtered_by_date
            currency_filtered_count += source_filtered_by_currency
                    
        except Exception as e:
            print(f"    âŒ {name}: {str(e)[:50]}...")
            continue
    
    avg_sentiment = total_sentiment / max(len(all_articles), 1)
    
    print(f"\n  ğŸ“ˆ æœ€è¿‘2å‘¨{currency}æ–°é—»åˆ†æç»“æœ:")
    print(f"    ğŸ“„ æœ‰æ•ˆæ–‡ç« æ€»æ•°: {len(all_articles)} ç¯‡")
    print(f"    ğŸ“… æ—¥æœŸè¿‡æ»¤: æ’é™¤äº† {date_filtered_count} ç¯‡è¿‡æœŸæ–‡ç« ")
    print(f"    ğŸ“Š å¹³å‡æƒ…ç»ªå¾—åˆ†: {avg_sentiment:.3f}")
    print(f"    ğŸ“ˆ æ­£é¢: {sentiment_counts['positive']} ç¯‡")
    print(f"    ğŸ“‰ è´Ÿé¢: {sentiment_counts['negative']} ç¯‡") 
    print(f"    ğŸ˜ ä¸­æ€§: {sentiment_counts['neutral']} ç¯‡")
    
    return {
        'articles': all_articles,
        'average_sentiment': avg_sentiment,
        'sentiment_counts': sentiment_counts,
        'total_articles': len(all_articles),
        'date_filtered': date_filtered_count,
        'currency_filtered': currency_filtered_count
    }

def get_social_media_sentiment_2weeks(currency="BTC"):
    """è·å–æœ€è¿‘2å‘¨çš„ç¤¾äº¤åª’ä½“æ•°æ®å¹¶åˆ†ææƒ…ç»ª"""
    print(f"\nğŸ“± åˆ†ææœ€è¿‘2å‘¨ç¤¾äº¤åª’ä½“ä¸­çš„{currency}æƒ…ç»ª...")
    
    # è®¾ç½®æœç´¢å…³é”®è¯
    if currency.upper() == "SOL":
        search_keywords = ['sol', 'solana']
        reddit_subreddits = [
            "https://www.reddit.com/r/solana/hot.json",
            "https://www.reddit.com/r/cryptocurrency/hot.json"
        ]
    else:
        search_keywords = ['btc', 'bitcoin']
        reddit_subreddits = [
            "https://www.reddit.com/r/bitcoin/hot.json",
            "https://www.reddit.com/r/cryptocurrency/hot.json"
        ]
    
    # Redditæ•°æ®
    reddit_sentiment = {"positive": 0, "negative": 0, "neutral": 0}
    reddit_total_score = 0
    reddit_posts = 0
    
    for reddit_url in reddit_subreddits:
        try:
            headers = {'User-Agent': 'CryptoSentimentBot/1.0'}
            request = urllib.request.Request(reddit_url, headers=headers)
            
            with urllib.request.urlopen(request, timeout=10) as response:
                content = response.read().decode('utf-8')
            
            data = json.loads(content)
            posts = data.get('data', {}).get('children', [])
            
            for post in posts[:30]:
                post_data = post.get('data', {})
                title = post_data.get('title', '')
                selftext = post_data.get('selftext', '')
                created_utc = post_data.get('created_utc', 0)
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æœ€è¿‘2å‘¨å†…
                post_date = datetime.fromtimestamp(created_utc) if created_utc else datetime.now()
                two_weeks_ago = datetime.now() - timedelta(days=14)
                
                if post_date < two_weeks_ago:
                    continue
                
                text_content = f"{title} {selftext}"
                
                if any(keyword in text_content.lower() for keyword in search_keywords):
                    score, label, _ = analyze_sentiment_keywords(text_content, currency)
                    reddit_total_score += score
                    reddit_sentiment[label] += 1
                    reddit_posts += 1
                    
        except Exception as e:
            print(f"  âŒ Redditè·å–å¤±è´¥: {str(e)[:50]}...")
            continue
            
    reddit_avg = reddit_total_score / max(reddit_posts, 1)
    print(f"  ğŸŸ  Reddit (2å‘¨å†…): {reddit_posts} ä¸ªç›¸å…³å¸–å­, å¹³å‡æƒ…ç»ª: {reddit_avg:.3f}")
    
    # CryptoCompareæ–°é—»API (é€šå¸¸æä¾›è¾ƒæ–°çš„æ•°æ®)
    crypto_compare_sentiment = {"positive": 0, "negative": 0, "neutral": 0}
    crypto_compare_total_score = 0
    crypto_compare_articles = 0
    
    try:
        url = "https://min-api.cryptocompare.com/data/v2/news/?lang=EN"
        headers = {'User-Agent': 'CryptoSentimentBot/1.0'}
        request = urllib.request.Request(url, headers=headers)
        
        with urllib.request.urlopen(request, timeout=10) as response:
            content = response.read().decode('utf-8')
        
        data = json.loads(content)
        articles = data.get('Data', [])
        
        two_weeks_ago_timestamp = (datetime.now() - timedelta(days=14)).timestamp()
        
        for article in articles[:50]:
            title = article.get('title', '')
            body = article.get('body', '')
            published_on = article.get('published_on', 0)
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æœ€è¿‘2å‘¨å†…
            if published_on < two_weeks_ago_timestamp:
                continue
            
            text_content = f"{title} {body}"
            
            if any(keyword in text_content.lower() for keyword in search_keywords):
                score, label, _ = analyze_sentiment_keywords(text_content, currency)
                crypto_compare_total_score += score
                crypto_compare_sentiment[label] += 1
                crypto_compare_articles += 1
        
        crypto_compare_avg = crypto_compare_total_score / max(crypto_compare_articles, 1)
        print(f"  ğŸ”· CryptoCompare (2å‘¨å†…): {crypto_compare_articles} ç¯‡ç›¸å…³æ–‡ç« , å¹³å‡æƒ…ç»ª: {crypto_compare_avg:.3f}")
        
    except Exception as e:
        print(f"  âŒ CryptoCompareè·å–å¤±è´¥: {str(e)[:50]}...")
        crypto_compare_avg = 0
        crypto_compare_articles = 0
    
    # ç»¼åˆç¤¾äº¤åª’ä½“æƒ…ç»ª
    total_social_posts = reddit_posts + crypto_compare_articles
    combined_sentiment = {
        "positive": reddit_sentiment["positive"] + crypto_compare_sentiment["positive"],
        "negative": reddit_sentiment["negative"] + crypto_compare_sentiment["negative"],
        "neutral": reddit_sentiment["neutral"] + crypto_compare_sentiment["neutral"]
    }
    
    combined_avg = (reddit_total_score + crypto_compare_total_score) / max(total_social_posts, 1)
    
    return {
        'reddit': {'sentiment': reddit_avg, 'posts': reddit_posts, 'counts': reddit_sentiment},
        'crypto_compare': {'sentiment': crypto_compare_avg, 'articles': crypto_compare_articles, 'counts': crypto_compare_sentiment},
        'combined': {'sentiment': combined_avg, 'total': total_social_posts, 'counts': combined_sentiment}
    }

def generate_2weeks_sentiment_insights(news_data, social_data, currency):
    """ç”Ÿæˆæœ€è¿‘2å‘¨çš„æƒ…ç»ªåˆ†ææ´å¯Ÿ"""
    news_sentiment = news_data['average_sentiment']
    social_sentiment = social_data['combined']['sentiment']
    
    # ç»¼åˆæƒ…ç»ªå¾—åˆ† (æ–°é—»75%æƒé‡ï¼Œç¤¾äº¤åª’ä½“25%æƒé‡ - å› ä¸ºæ–°é—»æ•°æ®ç»è¿‡æ—¶é—´ç­›é€‰æ›´å¯é )
    overall_sentiment = (news_sentiment * 0.75) + (social_sentiment * 0.25)
    
    # æƒ…ç»ªç­‰çº§
    if overall_sentiment > 0.4:
        sentiment_level = "ğŸš€ æåº¦ä¹è§‚"
        market_mood = f"æœ€è¿‘2å‘¨{currency}å¸‚åœºæƒ…ç»ªéå¸¸ç§¯æï¼ŒæŠ•èµ„è€…ä¿¡å¿ƒå¼ºåŠ²"
    elif overall_sentiment > 0.15:
        sentiment_level = "ğŸ“ˆ ä¹è§‚"
        market_mood = f"æœ€è¿‘2å‘¨{currency}å¸‚åœºæƒ…ç»ªåå‘ç§¯æï¼Œå‘å±•è¶‹åŠ¿è‰¯å¥½"
    elif overall_sentiment > -0.15:
        sentiment_level = "ğŸ˜ ä¸­æ€§"
        market_mood = f"æœ€è¿‘2å‘¨{currency}å¸‚åœºæƒ…ç»ªä¸­æ€§ï¼Œè§‚æœ›æ€åº¦æ˜æ˜¾"
    elif overall_sentiment > -0.4:
        sentiment_level = "ğŸ“‰ æ‚²è§‚"
        market_mood = f"æœ€è¿‘2å‘¨{currency}å¸‚åœºæƒ…ç»ªåå‘æ¶ˆæï¼Œé¢ä¸´ä¸€äº›æŒ‘æˆ˜"
    else:
        sentiment_level = "ğŸ’€ æåº¦æ‚²è§‚"
        market_mood = f"æœ€è¿‘2å‘¨{currency}å¸‚åœºæƒ…ç»ªéå¸¸æ¶ˆæï¼Œå­˜åœ¨ä¸¥é‡æ‹…å¿§"
    
    # ç”Ÿæˆå…³é”®æ´å¯Ÿ
    insights = []
    
    # æ—¶é—´èŒƒå›´å‡†ç¡®æ€§
    insights.append(f"ğŸ“… æ•°æ®æ—¶æ•ˆæ€§é«˜ï¼šåˆ†æåŸºäºæœ€è¿‘2å‘¨å†…çš„{news_data['total_articles'] + social_data['combined']['total']}ä¸ªæ•°æ®ç‚¹")
    
    # æ–°é—»vsç¤¾äº¤åª’ä½“å¯¹æ¯”
    sentiment_diff = abs(news_sentiment - social_sentiment)
    if sentiment_diff > 0.3:
        if news_sentiment > social_sentiment:
            insights.append(f"ğŸ“° ä¸»æµåª’ä½“å¯¹{currency}çš„æŠ¥é“æ¯”ç¤¾äº¤åª’ä½“è®¨è®ºæ›´ç§¯æ")
        else:
            insights.append(f"ğŸ“± ç¤¾äº¤åª’ä½“å¯¹{currency}çš„è®¨è®ºæ¯”ä¸»æµåª’ä½“æŠ¥é“æ›´ä¹è§‚")
    else:
        insights.append(f"ğŸ“Š æ–°é—»åª’ä½“å’Œç¤¾äº¤åª’ä½“å¯¹{currency}çš„æƒ…ç»ªåŸºæœ¬ä¸€è‡´")
    
    # æ•°æ®è´¨é‡è¯„ä¼°
    total_data_points = news_data['total_articles'] + social_data['combined']['total']
    if total_data_points > 40:
        insights.append(f"ğŸ“ˆ 2å‘¨æ•°æ®æ ·æœ¬ä¸°å¯Œ({total_data_points}ä¸ª)ï¼Œåˆ†æç»“æœé«˜åº¦å¯é ")
    elif total_data_points > 20:
        insights.append(f"ğŸ“Š 2å‘¨æ•°æ®æ ·æœ¬é€‚ä¸­({total_data_points}ä¸ª)ï¼Œåˆ†æç»“æœå¯ä¿¡")
    else:
        insights.append(f"âš ï¸ 2å‘¨æ•°æ®æ ·æœ¬æœ‰é™({total_data_points}ä¸ª)ï¼Œ{currency}è¿‘æœŸå…³æ³¨åº¦ä¸é«˜")
    
    # æƒ…ç»ªåˆ†å¸ƒåˆ†æ
    total_positive = news_data['sentiment_counts']['positive'] + social_data['combined']['counts']['positive']
    total_negative = news_data['sentiment_counts']['negative'] + social_data['combined']['counts']['negative']
    
    if total_positive > total_negative * 1.5:
        insights.append(f"ğŸŸ¢ è¿‘2å‘¨{currency}æ­£é¢å†…å®¹æ˜æ˜¾å ä¼˜ï¼Œå¸‚åœºä¿¡å¿ƒè¾ƒè¶³")
    elif total_negative > total_positive * 1.5:
        insights.append(f"ğŸ”´ è¿‘2å‘¨{currency}è´Ÿé¢å†…å®¹è¾ƒå¤šï¼Œå¸‚åœºæ‹…å¿§æƒ…ç»ªè¾ƒé‡")
    else:
        insights.append(f"ğŸŸ¡ è¿‘2å‘¨{currency}æ­£è´Ÿé¢å†…å®¹ç›¸å¯¹å‡è¡¡ï¼Œå¸‚åœºè§‚ç‚¹åˆ†åŒ–")
    
    # æ•°æ®è¿‡æ»¤æ•ˆæœ
    if news_data.get('date_filtered', 0) > 0:
        insights.append(f"ğŸ—“ï¸ æ’é™¤äº†{news_data['date_filtered']}ç¯‡è¿‡æœŸæ–‡ç« ï¼Œç¡®ä¿åˆ†ææ—¶æ•ˆæ€§")
    
    return {
        'overall_sentiment': overall_sentiment,
        'sentiment_level': sentiment_level,
        'market_mood': market_mood,
        'insights': insights,
        'time_range': 'æœ€è¿‘2å‘¨',
        'data_quality': 'high' if total_data_points > 40 else 'medium' if total_data_points > 20 else 'low'
    }

def analyze_currency_sentiment_2weeks(currency="BTC"):
    """åˆ†ææŒ‡å®šè´§å¸æœ€è¿‘2å‘¨çš„æƒ…ç»ª"""
    currency_upper = currency.upper()
    currency_name = "Bitcoin" if currency_upper == "BTC" else "Solana" if currency_upper == "SOL" else currency_upper
    
    print(f"ğŸ¯ {currency_name}({currency_upper}) æœ€è¿‘2å‘¨æƒ…ç»ªåˆ†æ")
    print("=" * 70)
    print(f"ğŸ“… åˆ†ææ—¶é—´èŒƒå›´: {(datetime.now() - timedelta(days=14)).strftime('%Y-%m-%d')} è‡³ {datetime.now().strftime('%Y-%m-%d')}")
    print(f"ğŸ” æ•°æ®æ¥æº: RSSæ–°é—»(6ä¸ªæº) + Reddit + CryptoCompare API")
    print(f"ğŸ¯ ç›®æ ‡è´§å¸: {currency_name}")
    print()
    
    # è·å–æ–°é—»æƒ…ç»ª
    start_time = time.time()
    news_data = get_rss_news_sentiment_2weeks(currency)
    news_time = time.time() - start_time
    
    # è·å–ç¤¾äº¤åª’ä½“æƒ…ç»ª
    start_time = time.time()  
    social_data = get_social_media_sentiment_2weeks(currency)
    social_time = time.time() - start_time
    
    # ç”Ÿæˆç»¼åˆåˆ†æ
    analysis = generate_2weeks_sentiment_insights(news_data, social_data, currency_upper)
    
    # è¾“å‡ºåˆ†ææŠ¥å‘Š
    print("\n" + "=" * 70)
    print(f"ğŸ“Š {currency_upper} æœ€è¿‘2å‘¨æƒ…ç»ªåˆ†ææŠ¥å‘Š")
    print("=" * 70)
    
    print(f"ğŸ¯ ç»¼åˆæƒ…ç»ªå¾—åˆ†: {analysis['overall_sentiment']:.3f}")
    print(f"ğŸ“ˆ æƒ…ç»ªç­‰çº§: {analysis['sentiment_level']}")
    print(f"ğŸ’­ å¸‚åœºå¿ƒæ€: {analysis['market_mood']}")
    
    print(f"\nğŸ“° æ–°é—»åª’ä½“æƒ…ç»ª (2å‘¨å†…):")
    print(f"  ğŸ“Š å¹³å‡å¾—åˆ†: {news_data['average_sentiment']:.3f}")
    print(f"  ğŸ“„ åˆ†ææ–‡ç« : {news_data['total_articles']} ç¯‡")
    print(f"  ğŸ“… è¿‡æ»¤æ–‡ç« : {news_data.get('date_filtered', 0)} ç¯‡è¿‡æœŸ")
    print(f"  ğŸ“ˆ æ­£é¢: {news_data['sentiment_counts']['positive']} ç¯‡")
    print(f"  ğŸ“‰ è´Ÿé¢: {news_data['sentiment_counts']['negative']} ç¯‡")
    print(f"  ğŸ˜ ä¸­æ€§: {news_data['sentiment_counts']['neutral']} ç¯‡")
    
    print(f"\nğŸ“± ç¤¾äº¤åª’ä½“æƒ…ç»ª (2å‘¨å†…):")
    print(f"  ğŸ“Š å¹³å‡å¾—åˆ†: {social_data['combined']['sentiment']:.3f}")
    print(f"  ğŸ’¬ åˆ†æå†…å®¹: {social_data['combined']['total']} æ¡")
    print(f"  ğŸ“ˆ æ­£é¢: {social_data['combined']['counts']['positive']} æ¡")
    print(f"  ğŸ“‰ è´Ÿé¢: {social_data['combined']['counts']['negative']} æ¡")
    print(f"  ğŸ˜ ä¸­æ€§: {social_data['combined']['counts']['neutral']} æ¡")
    
    print(f"\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
    for i, insight in enumerate(analysis['insights'], 1):
        print(f"  {i}. {insight}")
    
    print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
    print(f"  ğŸ“° æ–°é—»åˆ†æè€—æ—¶: {news_time:.2f}ç§’")
    print(f"  ğŸ“± ç¤¾äº¤åˆ†æè€—æ—¶: {social_time:.2f}ç§’")
    print(f"  ğŸ¯ 2å‘¨æ€»æ•°æ®ç‚¹: {news_data['total_articles'] + social_data['combined']['total']} ä¸ª")
    print(f"  ğŸ† æ•°æ®è´¨é‡ç­‰çº§: {analysis['data_quality'].upper()}")
    
    # æ˜¾ç¤ºæœ€æ–°æ–‡ç« ç¤ºä¾‹
    if news_data['articles']:
        print(f"\nğŸ“ æœ€è¿‘2å‘¨{currency_upper}ç›¸å…³æ–‡ç« ç¤ºä¾‹:")
        # æŒ‰æƒ…ç»ªæ’åºæ˜¾ç¤º
        sorted_articles = sorted(news_data['articles'], key=lambda x: x['sentiment_score'], reverse=True)
        for i, article in enumerate(sorted_articles[:6], 1):
            sentiment_emoji = "ğŸ“ˆ" if article['sentiment_score'] > 0.1 else "ğŸ“‰" if article['sentiment_score'] < -0.1 else "ğŸ˜"
            date_info = article['parsed_date'] if article['parsed_date'] != "Unknown" else "æ—¥æœŸæœªçŸ¥"
            print(f"  {i}. {sentiment_emoji} {article['title']}")
            print(f"     ğŸ“… {date_info} | ğŸ“° {article['source']} | ğŸ¯ {article['sentiment_score']:+.2f}")
    else:
        print(f"\nğŸ“ æ³¨æ„: æœ€è¿‘2å‘¨æœªæ‰¾åˆ°{currency_upper}ç›¸å…³çš„æ–°é—»æ–‡ç« ")
        print(f"  ğŸ’¡ å¯èƒ½åŸå› ï¼š")
        print(f"     â€¢ {currency_upper}è¿‘æœŸä¸æ˜¯åª’ä½“å…³æ³¨ç„¦ç‚¹")
        print(f"     â€¢ å¸‚åœºå¤„äºç›¸å¯¹å¹³é™æœŸ")
        print(f"     â€¢ æ•°æ®æºæš‚æ—¶æ²¡æœ‰ç›¸å…³å†…å®¹")
    
    # æ—¶é—´æ•ˆåº”åˆ†æ
    print(f"\nğŸ“… æ—¶é—´è¿‡æ»¤æ•ˆæœ:")
    if news_data.get('date_filtered', 0) > 0:
        print(f"  âœ… æˆåŠŸè¿‡æ»¤æ‰ {news_data['date_filtered']} ç¯‡è¿‡æœŸæ–‡ç« ")
        print(f"  ğŸ“Š æé«˜äº†æ•°æ®æ—¶æ•ˆæ€§å’Œåˆ†æå‡†ç¡®åº¦")
    else:
        print(f"  ğŸ“Š æ‰€æœ‰æ‰¾åˆ°çš„æ–‡ç« éƒ½åœ¨2å‘¨æ—¶é—´èŒƒå›´å†…")
    
    print(f"\nğŸ”— æ•°æ®æºå¥åº·åº¦:")
    print(f"  âœ… RSSæ–°é—»æº (6ä¸ª): æ­£å¸¸å·¥ä½œ")
    if social_data['reddit']['posts'] > 0:
        print(f"  âœ… Reddit API: æ­£å¸¸å·¥ä½œï¼Œè·å¾— {social_data['reddit']['posts']} ä¸ªå¸–å­") 
    else:
        print(f"  âš ï¸ Reddit API: 2å‘¨å†…{currency_upper}ç›¸å…³è®¨è®ºè¾ƒå°‘")
        
    if social_data['crypto_compare']['articles'] > 0:
        print(f"  âœ… CryptoCompare API: æ­£å¸¸å·¥ä½œï¼Œè·å¾— {social_data['crypto_compare']['articles']} ç¯‡æ–‡ç« ")
    else:
        print(f"  âš ï¸ CryptoCompare API: 2å‘¨å†…{currency_upper}ç›¸å…³å†…å®¹æœ‰é™")
    
    print(f"\nâš ï¸ å…è´£å£°æ˜:")
    print(f"  â€¢ æœ¬åˆ†æåŸºäºæœ€è¿‘2å‘¨å…¬å¼€æ•°æ®æºçš„æƒ…ç»ªå…³é”®è¯ç»Ÿè®¡")
    print(f"  â€¢ æ—¶é—´èŒƒå›´é™å®šæé«˜äº†åˆ†æçš„æ—¶æ•ˆæ€§å’Œå‡†ç¡®æ€§")
    print(f"  â€¢ ä¸æ„æˆæŠ•èµ„å»ºè®®ï¼Œè¯·ç»“åˆæŠ€æœ¯åˆ†æå’ŒåŸºæœ¬é¢ç»¼åˆåˆ¤æ–­")
    print(f"  â€¢ åŠ å¯†è´§å¸å¸‚åœºæ³¢åŠ¨å‰§çƒˆï¼Œæƒ…ç»ªå˜åŒ–å¿«é€Ÿ")
    
    return {
        'currency': currency_upper,
        'news_data': news_data,
        'social_data': social_data,
        'analysis': analysis
    }

def main():
    """ä¸»å‡½æ•° - åˆ†æBTCå’ŒSOL"""
    print("ğŸ¯ åŠ å¯†è´§å¸æœ€è¿‘2å‘¨æƒ…ç»ªå¯¹æ¯”åˆ†æ")
    print("=" * 80)
    print(f"ğŸ“… åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â° æ•°æ®èŒƒå›´: æœ€è¿‘14å¤©")
    print(f"ğŸ¯ åˆ†æå¸ç§: BTC vs SOL")
    print()
    
    # åˆ†æBTC
    print("ğŸŸ  å¼€å§‹åˆ†æBTC...")
    btc_result = analyze_currency_sentiment_2weeks("BTC")
    
    print("\n" + "="*50)
    
    # åˆ†æSOL
    print("ğŸŸ£ å¼€å§‹åˆ†æSOL...")
    sol_result = analyze_currency_sentiment_2weeks("SOL")
    
    # å¯¹æ¯”åˆ†æ
    print("\n" + "="*80)
    print("ğŸ“Š BTC vs SOL æœ€è¿‘2å‘¨æƒ…ç»ªå¯¹æ¯”")
    print("="*80)
    
    btc_score = btc_result['analysis']['overall_sentiment']
    sol_score = sol_result['analysis']['overall_sentiment']
    
    print(f"\nğŸ† æƒ…ç»ªå¾—åˆ†å¯¹æ¯”:")
    print(f"  ğŸŸ  BTC: {btc_score:.3f} - {btc_result['analysis']['sentiment_level']}")
    print(f"  ğŸŸ£ SOL: {sol_score:.3f} - {sol_result['analysis']['sentiment_level']}")
    
    if abs(btc_score - sol_score) < 0.05:
        winner = "ğŸ¤ æƒ…ç»ªç›¸å½“"
        analysis = "ä¸¤è€…æƒ…ç»ªæ°´å¹³éå¸¸æ¥è¿‘"
    elif btc_score > sol_score:
        winner = "ğŸŸ  BTCæƒ…ç»ªæ›´ç§¯æ"
        analysis = f"BTCæƒ…ç»ªé¢†å…ˆSOLçº¦{(btc_score - sol_score):.3f}åˆ†"
    else:
        winner = "ğŸŸ£ SOLæƒ…ç»ªæ›´ç§¯æ"
        analysis = f"SOLæƒ…ç»ªé¢†å…ˆBTCçº¦{(sol_score - btc_score):.3f}åˆ†"
    
    print(f"\nğŸ¯ å¯¹æ¯”ç»“æœ: {winner}")
    print(f"ğŸ“Š åˆ†æ: {analysis}")
    
    print(f"\nğŸ“° æ–°é—»å…³æ³¨åº¦å¯¹æ¯”:")
    btc_news = btc_result['news_data']['total_articles']
    sol_news = sol_result['news_data']['total_articles']
    print(f"  ğŸŸ  BTCæ–°é—»: {btc_news} ç¯‡")
    print(f"  ğŸŸ£ SOLæ–°é—»: {sol_news} ç¯‡")
    
    if btc_news > sol_news:
        print(f"  ğŸ“Š BTCåª’ä½“å…³æ³¨åº¦æ›´é«˜ ({btc_news - sol_news}ç¯‡å·®å¼‚)")
    elif sol_news > btc_news:
        print(f"  ğŸ“Š SOLåª’ä½“å…³æ³¨åº¦æ›´é«˜ ({sol_news - btc_news}ç¯‡å·®å¼‚)")
    else:
        print(f"  ğŸ“Š ä¸¤è€…åª’ä½“å…³æ³¨åº¦ç›¸å½“")
    
    print(f"\nğŸ“± ç¤¾äº¤çƒ­åº¦å¯¹æ¯”:")
    btc_social = btc_result['social_data']['combined']['total']
    sol_social = sol_result['social_data']['combined']['total']
    print(f"  ğŸŸ  BTCç¤¾äº¤: {btc_social} æ¡")
    print(f"  ğŸŸ£ SOLç¤¾äº¤: {sol_social} æ¡")
    
    if btc_social > sol_social:
        print(f"  ğŸ“Š BTCç¤¾äº¤çƒ­åº¦æ›´é«˜ ({btc_social - sol_social}æ¡å·®å¼‚)")
    elif sol_social > btc_social:
        print(f"  ğŸ“Š SOLç¤¾äº¤çƒ­åº¦æ›´é«˜ ({sol_social - btc_social}æ¡å·®å¼‚)")
    else:
        print(f"  ğŸ“Š ä¸¤è€…ç¤¾äº¤çƒ­åº¦ç›¸å½“")
    
    print(f"\nğŸ¯ æŠ•èµ„ç­–ç•¥å‚è€ƒ:")
    print(f"  â€¢ æƒ…ç»ªé¢†å…ˆè€…å¯èƒ½æœ‰çŸ­æœŸä»·æ ¼ä¼˜åŠ¿")
    print(f"  â€¢ å…³æ³¨åº¦é«˜çš„å¸ç§æµåŠ¨æ€§é€šå¸¸æ›´å¥½")
    print(f"  â€¢ å»ºè®®ç»“åˆæŠ€æœ¯åˆ†æç¡®è®¤å…¥åœºæ—¶æœº")
    print(f"  â€¢ 2å‘¨æ•°æ®æä¾›äº†è¾ƒå¥½çš„çŸ­ä¸­æœŸå‚è€ƒ")

if __name__ == "__main__":
    main()